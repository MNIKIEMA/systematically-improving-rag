{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 - Systematically Improving Your Rag Application\n",
    "\n",
    "> If you haven't ran the previous notebook `1. Evaluate Tools.ipynb`, please do so before continuing. A lot of the code in this notebook will be based off the evaluation methods that we cover in that notebook.\n",
    "\n",
    "# Generating Our Dataset\n",
    "\n",
    "We've defined a list of commands that we want to evaluate in our dataset in the `commands.yaml` file. In this specific notebook, we'll be generating a synthetic dataset of user requests that we can use to evaluate our mode's ability to call the commands that we've defined.\n",
    "\n",
    "Before we start generating our dataset, let's first understand what our model might struggle with. When it comes to deciding what tools to call, there are a few common failure points that a personal assistant might face.\n",
    "\n",
    "1. **A Lack Of Context** : It's common for users to use multiple note taking apps. We have Notion for company documents, Obsidian for personal notes and Apple Notes for quick notes that are more ephemeral. But it's difficult for a language model to know exactly which app to use when the user makes a request like `Sarah just got back to me on the project that our campaign was approved. can you make a note in our project page about it?` vs `I'm going shopping for groceries later, can you remind me that i need to buy milk, eggs and bread?`\n",
    "\n",
    "We might guess that the user might want notion but we can't assume that.\n",
    "\n",
    "2. **Multi-Step Tasks** : When we start scaling out our application to support calling multiple applications, we might find that our model struggles with multi-step tasks. For instance, given the request `create a new feature branch for the UI refactor and link all the tickets you've found about onboarding issues in the PR description`, we might find that our model struggles to call the required tools in the correct order or even call the tools at all.\n",
    "\n",
    "Being able to simulate these failure points in our synthetic dataset will help us to understand how our model performs in these scenarios. In this notebook, we'll do so in a few steps\n",
    "\n",
    "1. **Identifying Failure Points** : We'll first start by looking at how our model performs with simple/multi-step tasks to see how these failure points manifest themselves and start thinking about how we can generate synthetic queries that challenge our model which mimic these failure points\n",
    "\n",
    "2. **Generating Synthetic Questions** : We'll start by generating some initial queries. Once we're satisfied with the quality of the queries, we'll then generate more queries by sampling from these generated queries. We'll then use these queries to benchmark the precision and recall of our model's function calls and establish an initial baseline\n",
    "\n",
    "3. **Improving Performance** : Once we've established a simple baseline, we'll explore a few different strategies to improve the performance of our model. These include better descriptions of our commands, better prompts and some hard-coded few shot examples.\n",
    "\n",
    "Throughout this process, we'll be using `braintrust` to log all of our experiments so that it's easy for us to see the results of our experiments and share them with the rest of the team. \n",
    "\n",
    "Once we've done so in this notebook, we'll then proceed to evaluate other strategies such as dynamic retrieval of tools and few shot examples in a subsequent notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Failure Points\n",
    "\n",
    "Let's start by loading in our commands from the `commands.yaml` file. This contains a list of extensions and commands that we've manually cleaned and extracted from the Raycast Documentation.\n",
    "\n",
    "We've also added a small `description` field here that explains what our hypothetical users uses the extension for. This will help us to generate more challenging queries that require the model to understand the context of the user's request.\n",
    "\n",
    "When we generate synthetic queries, we want to randomly sample the commands to generate a diverse set of queries. Therefore, we want to flatten the dictionary of commands into a single flattened list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"I use obsidian to take down notes when I'm studying or taking online courses. I've been taking </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a variety of different courses on topics such as machine learning, marketing, copywriting etc. I'll use this </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">specific extension when I want to search for notes on a specific topic or ask questions about a topic. Sometimes I </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">might also want to create a quick dailyNote to log my progress for the day.\"</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'commands'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'searchNote'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Full-text search across all vault notes and their metadata'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dailyNote'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Create or open the daily note file in your configured daily notes folder'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'createNote'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Create a new markdown note in your specified vault location'</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'description'\u001b[0m: \u001b[32m\"I use obsidian to take down notes when I'm studying or taking online courses. I've been taking \u001b[0m\n",
       "\u001b[32ma variety of different courses on topics such as machine learning, marketing, copywriting etc. I'll use this \u001b[0m\n",
       "\u001b[32mspecific extension when I want to search for notes on a specific topic or ask questions about a topic. Sometimes I \u001b[0m\n",
       "\u001b[32mmight also want to create a quick dailyNote to log my progress for the day.\"\u001b[0m,\n",
       "    \u001b[32m'commands'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'name'\u001b[0m: \u001b[32m'searchNote'\u001b[0m, \u001b[32m'description'\u001b[0m: \u001b[32m'Full-text search across all vault notes and their metadata'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'dailyNote'\u001b[0m,\n",
       "            \u001b[32m'description'\u001b[0m: \u001b[32m'Create or open the daily note file in your configured daily notes folder'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'name'\u001b[0m: \u001b[32m'createNote'\u001b[0m, \u001b[32m'description'\u001b[0m: \u001b[32m'Create a new markdown note in your specified vault location'\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import yaml\n",
    "from rich import print\n",
    "\n",
    "with open(\"commands.yaml\", \"r\") as f:\n",
    "    commands = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "print(commands[\"extensions\"][\"obsidian\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'command_name': 'searchNote',\n",
       " 'extension_name': 'obsidian',\n",
       " 'extension_description': \"I use obsidian to take down notes when I'm studying or taking online courses. I've been taking a variety of different courses on topics such as machine learning, marketing, copywriting etc. I'll use this specific extension when I want to search for notes on a specific topic or ask questions about a topic. Sometimes I might also want to create a quick dailyNote to log my progress for the day.\",\n",
       " 'command_description': 'Full-text search across all vault notes and their metadata'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "def read_and_flatten_commands(file_path: str):\n",
    "    # Load and parse the commands.yaml file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        commands_data = yaml.safe_load(file)\n",
    "\n",
    "    # Extract extensions\n",
    "    extensions = commands_data[\"extensions\"]\n",
    "    commands = []\n",
    "\n",
    "    for extension_name, extension_data in extensions.items():\n",
    "        for command in extension_data[\"commands\"]:\n",
    "            commands.append(\n",
    "                {\n",
    "                    \"command_name\": command[\"name\"],\n",
    "                    \"extension_name\": extension_name,\n",
    "                    \"extension_description\": extension_data[\"description\"],\n",
    "                    \"command_description\": command[\"description\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return commands\n",
    "\n",
    "\n",
    "commands = read_and_flatten_commands(\"commands.yaml\")\n",
    "commands[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded in the commands, let's start by seeing how our model gets confused by the commands. Let's imagine we have three commands that we want to evalute\n",
    "\n",
    "- `obsidian.search`\n",
    "- `apple-notes.search`\n",
    "- `notion.search`\n",
    "\n",
    "Without much context or description of what the extension's command does, we might expect our model to get confused. This is ok because it provides a simple baseline to begin with. Let's see this in action below where our model gets the tool call wrong for every single query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Seems like the missing ingredient for the hamburger recipe was msg, let's add that in to our Beef Cheeseburger \n",
       "recipe note - Expected: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'apple-notes.search'</span><span style=\"font-weight: bold\">]</span> - Actual: obsidian.search\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Seems like the missing ingredient for the hamburger recipe was msg, let's add that in to our Beef Cheeseburger \n",
       "recipe note - Expected: \u001b[1m[\u001b[0m\u001b[32m'apple-notes.search'\u001b[0m\u001b[1m]\u001b[0m - Actual: obsidian.search\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What did I write about LSTMs previously? - Expected: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'obsidian.search'</span><span style=\"font-weight: bold\">]</span> - Actual: obsidian.search\n",
       "</pre>\n"
      ],
      "text/plain": [
       "What did I write about LSTMs previously? - Expected: \u001b[1m[\u001b[0m\u001b[32m'obsidian.search'\u001b[0m\u001b[1m]\u001b[0m - Actual: obsidian.search\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Can you note down that Jeanine is the new person in charge of the onboarding team in our team docs? - Expected: \n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'notion.search'</span><span style=\"font-weight: bold\">]</span> - Actual: obsidian.search\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Can you note down that Jeanine is the new person in charge of the onboarding team in our team docs? - Expected: \n",
       "\u001b[1m[\u001b[0m\u001b[32m'notion.search'\u001b[0m\u001b[1m]\u001b[0m - Actual: obsidian.search\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import instructor\n",
    "import openai\n",
    "from typing import Literal\n",
    "from rich import print\n",
    "\n",
    "client = instructor.from_openai(openai.OpenAI())\n",
    "\n",
    "commands = [\"obsidian.search\", \"apple-notes.search\", \"notion.search\"]\n",
    "queries = [\n",
    "    [\n",
    "        \"Seems like the missing ingredient for the hamburger recipe was msg, let's add that in to our Beef Cheeseburger recipe note\",\n",
    "        [\"apple-notes.search\"],\n",
    "    ],\n",
    "    [\"What did I write about LSTMs previously?\", [\"obsidian.search\"]],\n",
    "    [\n",
    "        \"Can you note down that Jeanine is the new person in charge of the onboarding team in our team docs?\",\n",
    "        [\"notion.search\"],\n",
    "    ],\n",
    "]\n",
    "\n",
    "for query, expected_tool in queries:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can call tools to help you with your tasks. You have access to the following tools: \"\n",
    "                + str(commands),\n",
    "            }\n",
    "        ],\n",
    "        response_model=Literal[\n",
    "            \"obsidian.search\", \"apple-notes.search\", \"notion.search\"\n",
    "        ],\n",
    "    )\n",
    "    print(f\"{query} - Expected: {expected_tool} - Actual: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model struggles to call the correct tool for each query. Implicitly, it seems to call obsidian for all of the queries. This would be quite unacceptible for a personal assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about multi-step tasks? Let's imagine we have the following commands added to our existing commands\n",
    "\n",
    "- `gmail.send_email`\n",
    "- `gmail.search`\n",
    "- `hubspot.create_contact`\n",
    "- `hubspot.search`\n",
    "- `hubspot.update_contact`\n",
    "\n",
    "Let's see how our model performs with a few commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "Hey can you send an email to Sarah about the new project we've been approved for and cc our client on it? The POC \n",
       "is hubert from Nvidia - \n",
       "Expected: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'gmail.send_email'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hubspot.search_contact'</span><span style=\"font-weight: bold\">]</span>\n",
       "Actual: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'gmail.search_contact'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hubspot.search_contact'</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "Hey can you send an email to Sarah about the new project we've been approved for and cc our client on it? The POC \n",
       "is hubert from Nvidia - \n",
       "Expected: \u001b[1m[\u001b[0m\u001b[32m'gmail.send_email'\u001b[0m, \u001b[32m'hubspot.search_contact'\u001b[0m\u001b[1m]\u001b[0m\n",
       "Actual: \u001b[1m[\u001b[0m\u001b[32m'gmail.search_contact'\u001b[0m, \u001b[32m'hubspot.search_contact'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "Send Rupert the booking confirmation for our Shinjuku hotel booking. It's inside my personal email at \n",
       "josh@gmail.com - \n",
       "Expected: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'gmail.search_email'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'gmail.search_contact'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'gmail.send_email'</span><span style=\"font-weight: bold\">]</span>\n",
       "Actual: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'gmail.search'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'gmail.search_contact'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hubspot.search_contact'</span><span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "Send Rupert the booking confirmation for our Shinjuku hotel booking. It's inside my personal email at \n",
       "josh@gmail.com - \n",
       "Expected: \u001b[1m[\u001b[0m\u001b[32m'gmail.search_email'\u001b[0m, \u001b[32m'gmail.search_contact'\u001b[0m, \u001b[32m'gmail.send_email'\u001b[0m\u001b[1m]\u001b[0m\n",
       "Actual: \u001b[1m[\u001b[0m\u001b[32m'gmail.search'\u001b[0m, \u001b[32m'gmail.search_contact'\u001b[0m, \u001b[32m'hubspot.search_contact'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import instructor\n",
    "import openai\n",
    "from typing import Literal\n",
    "\n",
    "client = instructor.from_openai(openai.OpenAI())\n",
    "\n",
    "commands = [\n",
    "    \"obsidian.search\",\n",
    "    \"apple-notes.search\",\n",
    "    \"notion.search\",\n",
    "    \"gmail.search_email\",\n",
    "    \"gmail.send_email\",\n",
    "    \"gmail.search_contact\",\n",
    "    \"gmail.search\",\n",
    "    \"hubspot.create_contact\",\n",
    "    \"hubspot.search_contact\",\n",
    "    \"hubspot.update_contact\",\n",
    "]\n",
    "queries = [\n",
    "    [\n",
    "        \"Hey can you send an email to Sarah about the new project we've been approved for and cc our client on it? The POC is hubert from Nvidia\",\n",
    "        [\"gmail.send_email\", \"hubspot.search_contact\"],\n",
    "    ],\n",
    "    [\n",
    "        \"Send Rupert the booking confirmation for our Shinjuku hotel booking. It's inside my personal email at josh@gmail.com\",\n",
    "        [\"gmail.search_email\", \"gmail.search_contact\", \"gmail.send_email\"],\n",
    "    ],\n",
    "]\n",
    "\n",
    "for query, expected_tool in queries:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can call tools to help you with your tasks. You have access to the following tools: \"\n",
    "                + str(commands),\n",
    "            }\n",
    "        ],\n",
    "        response_model=list[\n",
    "            Literal[\n",
    "                \"obsidian.search\",\n",
    "                \"apple-notes.search\",\n",
    "                \"notion.search\",\n",
    "                \"gmail.send_email\",\n",
    "                \"gmail.search_contact\",\n",
    "                \"gmail.search\",\n",
    "                \"hubspot.create_contact\",\n",
    "                \"hubspot.search_contact\",\n",
    "                \"hubspot.update_contact\",\n",
    "            ]\n",
    "        ],\n",
    "    )\n",
    "    print(f\"\\n\\n{query} - \" f\"\\nExpected: {expected_tool}\" f\"\\nActual: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is worse for the multi-step tasks. \n",
    "\n",
    "For the first and second example, we can see that it completely forgets to call the `gmail.send_email` task. It seems to default to search_contact for both gmail and hubspot by default instead of intuiting that hubspot should be called for professional contacts and gmail should be called for personal contacts.\n",
    "\n",
    "This is something that a few shot example could help with potentially or by users providing more context (either through a prompt or by updating the command/extension descriptions manually in their application).\n",
    "\n",
    "Now that we've identified some of these failure points, let's start generating our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Initial Queries\n",
    "\n",
    "Let's think about what we want to generate\n",
    "\n",
    "1. **Tool Calls** : Our queries should require 1-2 tools to be called\n",
    "2. **As user commands** : Most of these queries will be in the form of user queries (Eg. `send an email to Sarah about the Orion project and cc james`) that are more direct rather than conversational in nature (Eg. `Hey I hope you're doing well on your end. I was thinking that with the new updates to the orion project, we should probbaly cc sarah, what do you think?`)\n",
    "3. **Diverse** : We want to generate queries that span a diverse set of tone and lengths (Eg. we want queries that span from `send email sarah` to `hey can you send an email to Sarah, she's on my personal email`)\n",
    "\n",
    "Each test query will be a json object with the following fields\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"query\": str\n",
    "    \"labels\": list[str] # This is the `extension_name.command_name` of the tools that are called in the query - this helps us avoid issues where different extensions might have command names that are the same\n",
    "}\n",
    "```\n",
    "\n",
    "We'll write all of our queries to a `queries.jsonl` file and use that to store the queries we've generated. Since it's relatively simple to look at these queries in the default text editor, we'll just use that to eyeball and delete queries that don't make the cut.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'command_name': 'searchNote',\n",
       " 'extension_name': 'obsidian',\n",
       " 'extension_description': \"I use obsidian to take down notes when I'm studying or taking online courses. I've been taking a variety of different courses on topics such as machine learning, marketing, copywriting etc. I'll use this specific extension when I want to search for notes on a specific topic or ask questions about a topic. Sometimes I might also want to create a quick dailyNote to log my progress for the day.\",\n",
       " 'command_description': 'Full-text search across all vault notes and their metadata'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Load and parse the commands.yaml file\n",
    "with open(\"commands.yaml\", \"r\") as file:\n",
    "    commands_data = yaml.safe_load(file)\n",
    "\n",
    "# Extract extensions\n",
    "extensions = commands_data[\"extensions\"]\n",
    "commands = []\n",
    "\n",
    "for extension_name, extension_data in extensions.items():\n",
    "    for command in extension_data[\"commands\"]:\n",
    "        commands.append(\n",
    "            {\n",
    "                \"command_name\": command[\"name\"],\n",
    "                \"extension_name\": extension_name,\n",
    "                \"extension_description\": extension_data[\"description\"],\n",
    "                \"command_description\": command[\"description\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "commands[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator, ValidationInfo\n",
    "from asyncio import Semaphore\n",
    "import random\n",
    "\n",
    "\n",
    "class SyntheticQuery(BaseModel):\n",
    "    chain_of_thought: str\n",
    "    query: str\n",
    "    commands: list[str]\n",
    "\n",
    "    @field_validator(\"commands\")\n",
    "    def validate_commands(cls, v: list[str], info: ValidationInfo):\n",
    "        context = info.context\n",
    "        commands = context[\"commands\"]\n",
    "        command_names = set(\n",
    "            [\n",
    "                f\"{command['extension_name']}.{command['command_name']}\"\n",
    "                for command in commands\n",
    "            ]\n",
    "        )\n",
    "        for command in v:\n",
    "            if command not in command_names:\n",
    "                raise ValueError(f\"Command {command} not found in commands\")\n",
    "\n",
    "        return v\n",
    "\n",
    "\n",
    "client = instructor.from_openai(openai.AsyncOpenAI())\n",
    "\n",
    "\n",
    "async def generate_synthetic_query(\n",
    "    client: instructor.AsyncInstructor, commands: list[dict], sem: Semaphore\n",
    "):\n",
    "    tone = [\"natural\", \"curt and concise\", \"impatient\", \"slightly verbose\"]\n",
    "    async with sem:\n",
    "        resp = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"\"\"\n",
    "                Generate a natural user message that would require using one or more of the following commands:\n",
    "                <commands>\n",
    "                {% for command in commands %}\n",
    "                - {{ command['extension_name'] }}.{{ command['command_name'] }} : {{ command['command_description'] }}\n",
    "                {% endfor %}\n",
    "                </commands>\n",
    "\n",
    "                Requirements for the generated message:\n",
    "                - Do not explicitly mention any extension names or command names\n",
    "                - The message should clearly require using at least one of the commands' functionality\n",
    "                - Keep the message natural and conversational. It should be in the form of a message, remember that users are lazy and will type very short messages if possible.\n",
    "                - Focus on what the user is trying to achieve\n",
    "                - Have a tone of {{ tone }} with a length that's around {{ length }} words\n",
    "                - Make sure to have specific details in the query (Eg. the course, client name, project task achieved, specific feature being rolled out). These should be realistic (Eg. ACME Corp is a horrible name but Nike is a good one)\n",
    "                - The message can require multiple commands if it makes sense for the user's goal\n",
    "\n",
    "                <samples>\n",
    "                1. Can you help send an email to Sarah about the new project we've been approved for and cc our client on it? The POC is hubert from Nvidia\n",
    "                2. Create a new calendar event for the 18th of December at 10am\n",
    "                3. What's the temperature now in Taipei?\n",
    "                </samples>\n",
    "                \"\"\",\n",
    "                }\n",
    "            ],\n",
    "            context={\n",
    "                \"commands\": commands,\n",
    "                \"tone\": random.choice(tone),\n",
    "                \"length\": random.randint(5, 20),\n",
    "            },\n",
    "            response_model=SyntheticQuery,\n",
    "        )\n",
    "        return {\n",
    "            \"query\": resp.query,\n",
    "            \"labels\": resp.commands,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.asyncio import tqdm_asyncio as asyncio\n",
    "import json\n",
    "\n",
    "sem = Semaphore(10)\n",
    "with open(\"queries.jsonl\", \"a+\") as f:\n",
    "    chosen_commands = random.sample(commands, random.randint(1, 5))\n",
    "    coros = [generate_synthetic_query(client, chosen_commands, sem) for _ in range(10)]\n",
    "    results = await asyncio.gather(*coros)\n",
    "    for result in results:\n",
    "        f.write(json.dumps(result) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate some initial queries and add them to a `queries.jsonl` file. In my case, this generated the following queries.\n",
    "\n",
    "```bash\n",
    "{\n",
    "    \"query\": \"Create a note for today's meeting summary with Intel. Seems like they're keen on expanding in Asia and getting more into AI\",\n",
    "    \"labels\": [\"notion.quickCapture\"]\n",
    "}\n",
    "{\n",
    "    \"query\": \"Can you compile all of the open PRs that have been tagged under the UI refactor project and add them to a new page in Notion?\",\n",
    "    \"labels\": [\n",
    "        \"jira.searchTicket\",\n",
    "        \"github.searchPullRequests\", \n",
    "        \"notion.createPage\"\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Now that we've generated these queries, let's use them to seed our prompt with some few shot examples by randomly sampling a few queries from the `queries.jsonl` file each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "client = instructor.from_openai(openai.AsyncOpenAI())\n",
    "\n",
    "\n",
    "async def generate_synthetic_query(\n",
    "    client: instructor.AsyncInstructor,\n",
    "    commands: list[dict],\n",
    "    queries: list[dict],\n",
    "    sem: Semaphore,\n",
    "):\n",
    "    tone = [\"natural\", \"curt and concise\", \"impatient\", \"slightly verbose\"]\n",
    "    async with sem:\n",
    "        resp = await client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"\"\"\n",
    "                Generate a natural user message that would require using one or more of the following commands:\n",
    "                <commands>\n",
    "                {% for command in commands %}\n",
    "                - {{ command['extension_name'] }}.{{ command['command_name'] }} : {{ command['command_description'] }}\n",
    "                {% endfor %}\n",
    "                </commands>\n",
    "\n",
    "                Requirements for the generated message:\n",
    "                - Do not explicitly mention any extension names or command names\n",
    "                - The message should clearly require using at least one of the commands' functionality\n",
    "                - Keep the message natural and conversational. It should be in the form of a message, remember that users are lazy and will type very short messages if possible.\n",
    "                - Focus on what the user is trying to achieve\n",
    "                - Have a tone of {{ tone }} with a length that's around {{ length }} words\n",
    "                - Make sure to have specific details in the query (Eg. the course, client name, project task achieved, specific feature being rolled out). These should be realistic (Eg. ACME Corp is a horrible name but Nike is a good one)\n",
    "                - The message can require multiple commands if it makes sense for the user's goal\n",
    "\n",
    "                <samples>\n",
    "                {% for query in queries %}\n",
    "                - {{ query['query'] }} : {{ query['labels'] }}\n",
    "                {% endfor %}\n",
    "                </samples>\n",
    "                \"\"\",\n",
    "                }\n",
    "            ],\n",
    "            context={\n",
    "                \"commands\": commands,\n",
    "                \"tone\": random.choice(tone),\n",
    "                \"length\": random.randint(5, 20),\n",
    "                \"queries\": queries,\n",
    "            },\n",
    "            response_model=SyntheticQuery,\n",
    "        )\n",
    "        return {\n",
    "            \"query\": resp.query,\n",
    "            \"labels\": resp.commands,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:05<00:00,  2.83it/s]\n"
     ]
    }
   ],
   "source": [
    "sem = Semaphore(10)\n",
    "queries = [json.loads(line) for line in open(\"queries.jsonl\", \"r\")]\n",
    "\n",
    "with open(\"queries.jsonl\", \"a+\") as f:\n",
    "    coros = []\n",
    "\n",
    "    for _ in range(15):\n",
    "        chosen_commands = random.sample(commands, random.randint(1, 10))\n",
    "        chosen_queries = random.sample(queries, random.randint(1, 10))\n",
    "        coros.append(\n",
    "            generate_synthetic_query(client, chosen_commands, chosen_queries, sem)\n",
    "        )\n",
    "\n",
    "    results = await asyncio.gather(*coros)\n",
    "    for result in results:\n",
    "        f.write(json.dumps(result) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Precision and Recall\n",
    "\n",
    "Now that we've generated our dataset, we can start benchmarking the precision and recall of our model. We'll start by loading in the queries that we've generated and then seeing what the precion and recall of our model is. We've provided a function to evaluate the precision and recall of our model in a `helper.py` file that we defined in the`1. Evaluate Tools.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.33, 1.0)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers import calculate_precision, calculate_recall\n",
    "\n",
    "\n",
    "preds = [\"a\", \"b\", \"c\"]\n",
    "labels = [\"a\"]\n",
    "\n",
    "calculate_precision(preds, labels), calculate_recall(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that precision is 0.33 since only 1 item is relevant but 3 items were predicted. Recall is 1 since all of the relevant items ( in labels ) appeared in the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def validate_queries(queries: list[dict],commands: list[dict]):\n",
    "    valid_commands = set([f\"{command['extension_name']}.{command['command_name']}\" for command in commands])\n",
    "    for query in queries:\n",
    "        for label in query[\"labels\"]:\n",
    "            if label not in valid_commands:\n",
    "                raise ValueError(f\"Command {label} not found in commands\")\n",
    "    return True\n",
    "\n",
    "\n",
    "queries = [json.loads(line) for line in open(\"queries.jsonl\", \"r\")]\n",
    "commands = read_and_flatten_commands(\"commands.yaml\")\n",
    "validate_queries(queries, commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator, ValidationInfo\n",
    "import instructor\n",
    "\n",
    "\n",
    "class Commands(BaseModel):\n",
    "    commands: list[str]\n",
    "\n",
    "    @field_validator(\"commands\")\n",
    "    def validate_commands(cls, v: list[str], info: ValidationInfo):\n",
    "        context = info.context\n",
    "        commands = context[\"commands\"]\n",
    "        command_names = set(\n",
    "            [\n",
    "                f\"{command['extension_name']}.{command['command_name']}\"\n",
    "                for command in commands\n",
    "            ]\n",
    "        )\n",
    "        for command in v:\n",
    "            if command not in command_names:\n",
    "                raise ValueError(f\"Command {command} not found in commands\")\n",
    "\n",
    "        return v\n",
    "\n",
    "\n",
    "async def generate_commands(\n",
    "    query: str, client: instructor.AsyncInstructor, commands: list[dict]\n",
    "):\n",
    "    resp =  await client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "                You are a helpful assistant that can call tools to help you with your tasks. Only use the following commands provided below.\n",
    "\n",
    "                <commands>\n",
    "                {% for command in commands %}\n",
    "                - {{ command['extension_name'] }}.{{ command['command_name'] }}\n",
    "                {% endfor %}\n",
    "                </commands>\n",
    "                \"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        response_model=Commands,\n",
    "        context={\"commands\": commands},\n",
    "    )\n",
    "    return resp.commands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment week-6-1734443274 is running at https://www.braintrust.dev/app/567/p/function-calling/experiments/week-6-1734443274\n",
      "function-calling (data): 44it [00:00, 189475.75it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759cb2c8e45f4b62846fb8578af4f9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "function-calling (tasks):   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "week-6-1734443274 compared to week-6-1734443256:\n",
      "70.82% (-06.82%) 'recall'    score\t(2 improvements, 5 regressions)\n",
      "67.80% (-06.43%) 'precision' score\t(2 improvements, 6 regressions)\n",
      "\n",
      "0.75s (-09.25%) 'duration'\t(17 improvements, 27 regressions)\n",
      "\n",
      "See results for week-6-1734443274 at https://www.braintrust.dev/app/567/p/function-calling/experiments/week-6-1734443274\n"
     ]
    }
   ],
   "source": [
    "from braintrust import Eval, Score\n",
    "import openai\n",
    "\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    return [\n",
    "        Score(\n",
    "            name=\"precision\",\n",
    "            score=calculate_precision(kwargs[\"expected\"], output),\n",
    "        ),\n",
    "        Score(\n",
    "            name=\"recall\",\n",
    "            score=calculate_recall(kwargs[\"expected\"], output),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "client = instructor.from_openai(openai.AsyncOpenAI())   \n",
    "commands = read_and_flatten_commands(\"commands.yaml\")\n",
    "\n",
    "async def task(query: str, hooks):\n",
    "    return await generate_commands(query, client, commands)\n",
    "\n",
    "base_case = await Eval(\n",
    "    \"function-calling\",\n",
    "    data=lambda: [\n",
    "        {\n",
    "            \"input\": row[\"query\"],\n",
    "            \"expected\": row[\"labels\"],\n",
    "        }\n",
    "        for row in queries\n",
    "    ],\n",
    "    task=task,\n",
    "    max_concurrency=10,\n",
    "    scores=[evaluate_braintrust],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Performance\n",
    "\n",
    "## Adding Detailed Descriptions\n",
    "\n",
    "One of the common failure points that we identified was that given a simple function name, our model would struggle to call the correct tool. Let's try to see if we can improve the performance of our model by adding more detailed descriptions to our commands.\n",
    "\n",
    "Luckily, we've already done so in the `commands.yaml` file. Let's see if this improves the performance of our model. Since all this requires is modifying the jinja templating in our original file, minimal changes are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment week-6-1734443256 is running at https://www.braintrust.dev/app/567/p/function-calling/experiments/week-6-1734443256\n",
      "function-calling (data): 44it [00:00, 24136.72it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a87bc5fdaa44e49ea8cf7300909c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "function-calling (tasks):   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "week-6-1734443256 compared to week-6-1734443170:\n",
      "77.64% (-10.61%) 'recall'    score\t(3 improvements, 7 regressions)\n",
      "74.23% (-11.00%) 'precision' score\t(1 improvements, 7 regressions)\n",
      "\n",
      "0.84s (+11.78%) 'duration'\t(30 improvements, 14 regressions)\n",
      "\n",
      "See results for week-6-1734443256 at https://www.braintrust.dev/app/567/p/function-calling/experiments/week-6-1734443256\n"
     ]
    }
   ],
   "source": [
    "async def generate_commands_with_descriptions(\n",
    "    query: str, client: instructor.AsyncInstructor, commands: list[dict]\n",
    "):\n",
    "    resp =  await client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "                You are a helpful assistant that can execute command(s) listed below to help answer a user query.\n",
    "\n",
    "                Only use the following commands provided below. A description of each command and its name is provided to help you understand what executing the command will do.\n",
    "                \n",
    "                <commands>\n",
    "                {% for command in commands %}\n",
    "                - Command Name : {{ command['extension_name'] }}.{{ command['command_name'] }}\n",
    "                  Description  : ({{ command['command_description'] }})\n",
    "                {% endfor %}\n",
    "                </commands>\n",
    "                \"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        response_model=Commands,\n",
    "        context={\"commands\": commands},\n",
    "    )\n",
    "    return resp.commands\n",
    "\n",
    "client = instructor.from_openai(openai.AsyncOpenAI())   \n",
    "commands = read_and_flatten_commands(\"commands.yaml\")\n",
    "\n",
    "async def task(query: str, hooks):\n",
    "    return await generate_commands_with_descriptions(query, client, commands)\n",
    "\n",
    "descriptions = await Eval(\n",
    "    \"function-calling\",\n",
    "    data=lambda: [\n",
    "        {\n",
    "            \"input\": row[\"query\"],\n",
    "            \"expected\": row[\"labels\"],\n",
    "        }\n",
    "        for row in queries\n",
    "    ],\n",
    "    task=task,\n",
    "    max_concurrency=10,\n",
    "    scores=[evaluate_braintrust],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Few Shot Examples\n",
    "\n",
    "If we were to deploy this in production, we could also imagine that we'd want to add some few shot examples. We want to do so because it allows the model to be able to use those few shot examples to understand the context of the user's request.\n",
    "\n",
    "In a production application, we might obtain these examples by saving user queries and the tools they selected in the end in a local vector database. When the user makes a request, we'd then retrieve the most similar examples and include them in the prompt. \n",
    "\n",
    "One piece of low hanging fruit is providing context for which note taking app to choose - `obsidian`, `notion` or `apple-notes`. We'll add a few shot example to our prompt for each so our model can use this context to make a better decision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment week-6-1734443301 is running at https://www.braintrust.dev/app/567/p/function-calling/experiments/week-6-1734443301\n",
      "function-calling (data): 44it [00:00, 36880.37it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b39becd590401f9bc64b57586f6c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "function-calling (tasks):   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "week-6-1734443301 compared to week-6-1734443274:\n",
      "88.25% (+17.43%) 'recall'    score\t(11 improvements, 4 regressions)\n",
      "87.50% (+19.70%) 'precision' score\t(12 improvements, 3 regressions)\n",
      "\n",
      "1.24s (+48.82%) 'duration'\t(20 improvements, 24 regressions)\n",
      "\n",
      "See results for week-6-1734443301 at https://www.braintrust.dev/app/567/p/function-calling/experiments/week-6-1734443301\n"
     ]
    }
   ],
   "source": [
    "async def generate_commands_with_few_shots(\n",
    "    query: str, client: instructor.AsyncInstructor, commands: list[dict]\n",
    "):\n",
    "    resp =  await client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "                You are a helpful assistant that can execute command(s) listed below to help answer a user query.\n",
    "\n",
    "                Only use the following commands provided below. A description of each command and its name is provided to help you understand what executing the command will do.\n",
    "                \n",
    "                <commands>\n",
    "                {% for command in commands %}\n",
    "                - Command Name : {{ command['extension_name'] }}.{{ command['command_name'] }}\n",
    "                  Description  : ({{ command['command_description'] }})\n",
    "                {% endfor %}\n",
    "                </commands>\n",
    "\n",
    "                <examples>\n",
    "                User: Create a new page for today's meeting with Asus\n",
    "                Assistant: [notion.createPage]\n",
    "\n",
    "                User: quick brain dump on the ASUS project to be added to the existing notes. I think they've got a huge amount of plans involving them expanding in europe. Specifically what I think is important in this specific case is that we need to work on building that relationship early on so that we become the company they always talk to. \n",
    "                Assistant: [notion.quickCapture]\n",
    "\n",
    "                User: Get my notes on CS1423 from the last lecture\n",
    "                Assistant: [obsidian.search]\n",
    "\n",
    "                User: Can you send an email to Sarah about the new project we've been approved for and cc our client on it? The POC is hubert from Nvidia\n",
    "                Assistant: [gmail.send_email, hubspot.search_contact]\n",
    "\n",
    "                User: Can you fetch the onboarding documents for the interns to use?\n",
    "                Assistant: [notion.search]\n",
    "\n",
    "                User: Create a new document to summarize the actionables from the meeting with Asus\n",
    "                Assistant: [notion.createPage]\n",
    "\n",
    "                User: Can you pull up the meeting notes from the meeting with Thoughtworks?\n",
    "                Assistant: [notion.search]\n",
    "\n",
    "                User: What items did I need to buy for dinner tonight?\n",
    "                Assistant: [apple-notes.search]\n",
    "\n",
    "                User: Fetch my notes on CS4123 from the last lecture\n",
    "                Assistant: [obsidian.search]\n",
    "                </examples>\n",
    "                \"\"\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        response_model=Commands,\n",
    "        context={\"commands\": commands},\n",
    "    )\n",
    "    return resp.commands\n",
    "\n",
    "client = instructor.from_openai(openai.AsyncOpenAI())   \n",
    "commands = read_and_flatten_commands(\"commands.yaml\")\n",
    "\n",
    "async def task(query: str, hooks):\n",
    "    return await generate_commands_with_few_shots(query, client, commands)\n",
    "\n",
    "few_shots = await Eval(\n",
    "    \"function-calling\",\n",
    "    data=lambda: [\n",
    "        {\n",
    "            \"input\": row[\"query\"],\n",
    "            \"expected\": row[\"labels\"],\n",
    "        }\n",
    "        for row in queries\n",
    "    ],\n",
    "    task=task,\n",
    "    max_concurrency=10,\n",
    "    scores=[evaluate_braintrust],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Approach</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base Case</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Descriptions</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Few Shots</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Approach  recall  precision\n",
       "0     Base Case    0.71       0.68\n",
       "1  Descriptions    0.78       0.74\n",
       "2     Few Shots    0.88       0.88"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = [\n",
    "    [\"Base Case\" ,base_case],\n",
    "    [\"Descriptions\" ,descriptions],\n",
    "    [\"Few Shots\" ,few_shots]\n",
    "]\n",
    "\n",
    "scores = []\n",
    "for label, item in results:\n",
    "    item_score = {\n",
    "        metric: item.summary.scores[metric].score  for metric in item.summary.scores\n",
    "    }\n",
    "    scores.append({\n",
    "        \"Approach\": label,\n",
    "        **item_score\n",
    "    })\n",
    "\n",
    "pd.DataFrame(scores).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "| Approach | Recall | Precision |\n",
    "| -------- | --------- | ------ |\n",
    "| Original | 0.71| 0.68 |\n",
    "| With Descriptions | 0.78 | 0.74 |\n",
    "| With Few Shots | 0.88 | 0.88 |\n",
    "\n",
    "We can see that adding descriptions and few shots to our prompt has improved the precision and recall of our model. This is because it increases the context of the user's request and gives the model more examples to learn from. \n",
    "\n",
    "In the next notebook, we'll see how we can fetch relevant examples and tools from a vector database while ensuring performance remains high. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
