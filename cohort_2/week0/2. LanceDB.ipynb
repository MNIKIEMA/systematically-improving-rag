{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LanceDB\n",
    "\n",
    "> **Important**: It's important to note that we'll be using the local version of LanceDB in this course. This code can also work with the hosted version of LanceDB as long as you change the connection string to point to your LanceDB instance. \n",
    "\n",
    "LanceDB is a vector database that makes it easy to build and evaluate RAG applications. In this notebook, we'll explore how to use LanceDB's key features to store, search, and retrieve data effectively.\n",
    "\n",
    "## Why this Matters\n",
    "When building RAG systems, choosing the right vector database is crucial. While many options exist, LanceDB stands out by providing:\n",
    "\n",
    "1. Simple schema definition with Pydantic models\n",
    "2. Automatic embedding generation and management\n",
    "3. A unified API for different search types (vector, full-text, hybrid)\n",
    "\n",
    "## What you'll Learn\n",
    "\n",
    "Through hands-on examples, you'll discover how to:\n",
    "\n",
    "1. Set up LanceDB tables with proper schemas\n",
    "2. Perform different types of searches (full-text, vector, hybrid)\n",
    "3. Enhance results with reranking\n",
    "\n",
    "By the end of this notebook, you'll understand how to leverage LanceDB's capabilities to build robust retrieval systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up LanceDB\n",
    "\n",
    "We can create our LanceDB instance using the `lancedb` library and the `connect` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanceDBConnection(uri='c:\\\\OpenSource\\\\systematically-improving-rag\\\\cohort_2\\\\week0\\\\lancedb')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should in turn create a `lancedb` directory in your current working directory. We can validate that this is the case by running the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.path.exists(\"./lancedb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our first table. We'll do so by defining a Pydantic Schema and then using the `Table` class to create our table. We'll also use the OpenAI Embeddings API to create embeddings for these individual documents that we ingest. \n",
    "\n",
    "To read more about the different embedding models that are available for use, you can check out their documentation [here](https://lancedb.github.io/lancedb/embeddings/available_embedding_models/text_embedding_functions/). \n",
    "\n",
    "First let's define our Table schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959704bf01984ee6b1bae2e3ac942374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febb9f9721cd4dd4b42210e8f329f69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f579cb646034b729791718aff03c68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c7803250134b89a1b409cd6899d377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa280e519b2141ccaedb1f0ab2401c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40d2a2333344caf89046ac1ee3c9c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab299190f3345b9bda1b17b97c8a3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3fb120549141f2b390adf53ccba67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d12dc3904694a66a0ecc3d277ec4a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71125c9886d64fce832ec2e2776bdeae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03a2722f1c74d07802c8b82c43acc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "\n",
    "func = get_registry().get(\"sentence-transformers\").create(name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                                          device=\"cpu\")\n",
    "\n",
    "\n",
    "# Define a Schema\n",
    "class Words(LanceModel):\n",
    "    # This is the source field that will be used as input to the OpenAI Embedding API\n",
    "    text: str = func.SourceField()\n",
    "\n",
    "    # This is the vector field that will store the output of the OpenAI Embedding API\n",
    "    vector: Vector(func.ndims()) = func.VectorField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our table with this schema. By using Pydantic, LanceDB will create the necessary fields for us and we can use the `add` method to ingest our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = db.create_table(\"words\", schema=Words, mode=\"overwrite\")\n",
    "\n",
    "# Ingest our data\n",
    "table.add([{\"text\": \"hello world\"}, {\"text\": \"goodbye world\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that our data was ingested correctly and that the embeddings were created by converting our table to a pandas dataframe and printing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "vector",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f9f85a71-1cf6-4d46-9b24-89a8c2121ba3",
       "rows": [
        [
         "0",
         "hello world",
         "[-3.44772302e-02  3.10232416e-02  6.73502078e-03  2.61090286e-02\n -3.93620469e-02 -1.60302475e-01  6.69240206e-02 -6.44146837e-03\n -4.74505499e-02  1.47588830e-02  7.08752796e-02  5.55275492e-02\n  1.91933382e-02 -2.62513719e-02 -1.01095177e-02 -2.69405339e-02\n  2.23074388e-02 -2.22266000e-02 -1.49692625e-01 -1.74931269e-02\n  7.67632248e-03  5.43522798e-02  3.25447018e-03  3.17259803e-02\n -8.46214220e-02 -2.94060372e-02  5.15956841e-02  4.81240563e-02\n -3.31480894e-03 -5.82792163e-02  4.19692881e-02  2.22107247e-02\n  1.28188848e-01 -2.23389566e-02 -1.16561800e-02  6.29283264e-02\n -3.28762904e-02 -9.12260860e-02 -3.11754029e-02  5.26995808e-02\n  4.70348559e-02 -8.42030644e-02 -3.00561562e-02 -2.07447875e-02\n  9.51779541e-03 -3.72173614e-03  7.34330574e-03  3.93242612e-02\n  9.32740867e-02 -3.78858973e-03 -5.27421497e-02 -5.80581240e-02\n -6.86440105e-03  5.28323557e-03  8.28929767e-02  1.93627384e-02\n  6.28452096e-03 -1.03308111e-02  9.03236959e-03 -3.76838073e-02\n -4.52060997e-02  2.40163468e-02 -6.94418326e-03  1.34916240e-02\n  1.00054950e-01 -7.16838464e-02 -2.16950607e-02  3.16184647e-02\n -5.16346134e-02 -8.22476670e-02 -6.56932816e-02 -9.89539269e-03\n  5.81640052e-03  7.35545158e-02 -3.40503119e-02  2.48861127e-02\n  1.44880731e-02  2.64574047e-02  9.65674408e-03  3.02173309e-02\n  5.28039709e-02 -7.53598958e-02  9.89719946e-03  2.98368130e-02\n  1.75555777e-02  2.30920166e-02  1.93384557e-03  1.40018540e-03\n -4.71759699e-02 -1.11943660e-02 -1.14201397e-01 -1.98120009e-02\n  4.02661972e-02  2.19298364e-03 -7.97922164e-02 -2.53823232e-02\n  9.44829285e-02 -2.89811641e-02 -1.45002529e-01  2.30977431e-01\n  2.77311541e-02  3.21114659e-02  3.10650095e-02  4.28328216e-02\n  6.42378256e-02  3.21632214e-02 -4.87668021e-03  5.56994677e-02\n -3.75323780e-02 -2.15056278e-02 -2.83426829e-02 -2.88469288e-02\n  3.83530855e-02 -1.74686927e-02  5.24853058e-02 -7.48760179e-02\n -3.12598050e-02  2.18415316e-02 -3.98957059e-02 -8.58711265e-03\n  2.69566141e-02 -4.84954789e-02  1.14698401e-02  2.96182502e-02\n -2.05721948e-02  1.31039126e-02  2.88333986e-02 -3.19419908e-33\n  6.47820085e-02 -1.81301814e-02  5.17899320e-02  1.21982709e-01\n  2.87802033e-02  8.72197468e-03 -7.05211982e-02 -1.69072542e-02\n  4.07397263e-02  4.21161950e-02  2.54472289e-02  3.57462279e-02\n -4.91447710e-02  2.12909584e-03 -1.55465892e-02  5.07306196e-02\n -4.81852964e-02  3.58806141e-02 -4.06706287e-03  1.01724736e-01\n -5.59700131e-02 -1.06809987e-02  1.12358425e-02  9.06865373e-02\n  4.23446530e-03  3.51386257e-02 -9.70288273e-03 -9.38652232e-02\n  9.28555429e-02  8.00495502e-03 -7.70539185e-03 -5.20867445e-02\n -1.25879683e-02  3.26687912e-03  6.01355266e-03  7.58156599e-03\n  1.05171353e-02 -8.63455161e-02 -6.98787421e-02 -2.53385329e-03\n -9.09765661e-02  4.68873270e-02  5.20765074e-02  7.19392858e-03\n  1.09035699e-02 -5.22948848e-03  1.39373550e-02  2.19683088e-02\n  3.42085473e-02  6.02246821e-02  1.16633877e-04  1.47319334e-02\n -7.00892210e-02  2.84990277e-02 -2.76016146e-02  1.07683856e-02\n  3.48309614e-02 -2.24878900e-02  9.76908207e-03  7.72278085e-02\n  2.15883516e-02  1.14956193e-01 -6.80011138e-02  2.37609260e-02\n -1.59839224e-02 -1.78269744e-02  6.43949434e-02  3.20257135e-02\n  5.02702594e-02 -5.91366040e-03 -3.37080136e-02  1.78403780e-02\n  1.65733974e-02  6.32965863e-02  3.46771926e-02  4.64734510e-02\n  9.79061350e-02 -6.63548615e-03  2.52070613e-02 -7.79883936e-02\n  1.69264432e-02 -9.45848064e-04  2.24719252e-02 -3.82531881e-02\n  9.57047790e-02 -5.35076950e-03  1.04690231e-02 -1.15240507e-01\n -1.32625327e-02 -1.07093938e-02 -8.31172243e-02  7.32735693e-02\n  4.93921936e-02 -8.99438001e-03 -9.58455950e-02  3.36614783e-33\n  1.24931790e-01  1.93497073e-02 -5.82257658e-02 -3.59882936e-02\n -5.07467799e-02 -4.56623398e-02 -8.26034769e-02  1.48194820e-01\n -8.84211212e-02  6.02744184e-02  5.10302745e-02  1.03031266e-02\n  1.41214252e-01  3.08138188e-02  6.10330999e-02 -5.28512336e-02\n  1.36648849e-01  9.18991491e-03 -1.73253044e-02 -1.28486445e-02\n -7.99527019e-03 -5.09801172e-02 -5.23506403e-02  7.59302918e-03\n -1.51662538e-02  1.69603471e-02  2.12705173e-02  2.05579773e-02\n -1.20028004e-01  1.44617893e-02  2.67598853e-02  2.53305510e-02\n -4.27546501e-02  6.76842313e-03 -1.44585725e-02  4.52620685e-02\n -9.14765671e-02 -1.94391757e-02 -1.78334825e-02 -5.49101084e-02\n -5.26410975e-02 -1.04590310e-02 -5.20160198e-02  2.08921209e-02\n -7.99702927e-02 -1.21112876e-02 -5.77313937e-02  2.31782850e-02\n -8.03167373e-03 -2.59892549e-02 -7.99566731e-02 -2.07288396e-02\n  4.88176718e-02 -2.03891676e-02 -4.91766408e-02  1.41596710e-02\n -6.36221096e-02 -7.80743407e-03  1.64315403e-02 -2.56825667e-02\n  1.33810826e-02  2.62487624e-02  9.97834839e-03  6.32289425e-02\n  2.67210999e-03 -6.58275606e-03  1.66320149e-02  3.23664397e-02\n  3.79425324e-02 -3.63760442e-02 -6.91094948e-03  1.59632938e-04\n -1.63355214e-03 -2.72782370e-02 -2.80380379e-02  4.96815518e-02\n -2.88671814e-02 -2.41811038e-03  1.47748971e-02  9.76453628e-03\n  5.79755846e-03  1.34861283e-02  5.56793204e-03  3.72271016e-02\n  7.23251654e-03  4.01562564e-02  8.15031976e-02  7.19916373e-02\n -1.30561590e-02 -4.28820550e-02 -1.10112214e-02  4.89782263e-03\n -9.22976062e-03  3.51914465e-02 -5.10349944e-02 -1.57143756e-08\n -8.86244252e-02  2.39093099e-02 -1.62387323e-02  3.17004174e-02\n  2.72841677e-02  5.24687953e-02 -4.70709540e-02 -5.88474683e-02\n -6.32082149e-02  4.08885591e-02  4.98279296e-02  1.06551699e-01\n -7.45023191e-02 -1.24954656e-02  1.83706097e-02  3.94741707e-02\n -2.47978605e-02  1.45163005e-02 -3.70692201e-02  2.00157147e-02\n -4.86252029e-05  9.86653473e-03  2.48387456e-02 -5.24580590e-02\n  2.93141045e-02 -8.71919543e-02 -1.44997444e-02  2.60190777e-02\n -1.87464189e-02 -7.62051642e-02  3.50432731e-02  1.03639498e-01\n -2.80505344e-02  1.27182547e-02 -7.63254985e-02 -1.86524093e-02\n  2.49766875e-02  8.14453512e-02  6.87588975e-02 -6.40566126e-02\n -8.38939324e-02  6.13623932e-02 -3.35455798e-02 -1.06153369e-01\n -4.00805473e-02  3.25302258e-02  7.66248778e-02 -7.30161443e-02\n  3.37591657e-04 -4.08715717e-02 -7.57884309e-02  2.75276508e-02\n  7.46254250e-02  1.77172981e-02  9.12183523e-02  1.10220164e-01\n  5.69832919e-04  5.14633358e-02 -1.45512763e-02  3.32319289e-02\n  2.37922743e-02 -2.28897817e-02  3.89375687e-02  3.02068386e-02]"
        ],
        [
         "1",
         "goodbye world",
         "[ 2.27455497e-02  8.00969675e-02  3.30892578e-02  1.06781523e-03\n  4.23904210e-02 -5.19888997e-02  3.92646119e-02 -8.45222250e-02\n  4.61245552e-02 -3.56179886e-02  3.04195192e-02 -2.37092539e-03\n  7.35828234e-03  4.31711935e-02 -8.65970626e-02  5.35937808e-02\n -4.43696044e-02 -1.00906594e-02 -1.16498575e-01 -2.49666255e-02\n -4.83752601e-02  6.80495352e-02 -1.16188405e-02  5.09177893e-02\n -5.84151633e-02  8.84107128e-02  1.93398539e-02  2.57968269e-02\n  5.37607074e-03 -3.77639197e-02  1.52272312e-02  4.00342345e-02\n  2.25410843e-03 -6.09549619e-02  2.83243079e-02  1.64233781e-02\n -4.11423370e-02 -7.86902010e-02 -2.14100201e-02 -1.18745100e-02\n  2.19878741e-02  4.69609722e-03 -3.59995589e-02 -3.07005979e-02\n -2.40690596e-02  1.96674317e-02 -6.12896532e-02 -6.40293658e-02\n  6.48113638e-02  9.46378484e-02  4.49264012e-02 -3.42617370e-02\n -3.62833925e-02  1.66060142e-02  6.34100661e-02 -1.14163179e-02\n  1.06206782e-01 -2.98267789e-02  4.42609470e-03 -2.31928192e-02\n  1.20007778e-02 -2.61807963e-02 -2.77909283e-02  2.05433648e-02\n  1.23890750e-01  4.07247059e-03  1.28522618e-02  6.95450008e-02\n -4.50102836e-02 -9.51666385e-03 -9.84865203e-02 -1.48595814e-02\n  1.95055045e-02  3.79599407e-02 -9.74673778e-04  4.93689477e-02\n -7.37279048e-03 -3.28335129e-02 -1.86330117e-02  5.00260703e-02\n  1.34544224e-01 -3.88516374e-02 -1.74129885e-02 -4.80064973e-02\n -2.84675919e-02  4.44596680e-03 -3.32663693e-02 -3.85422371e-02\n  4.97881286e-02 -3.27481031e-02 -9.10985246e-02 -1.59686874e-03\n  6.06144033e-02  3.92315835e-02 -7.56219625e-02 -6.21106811e-02\n  3.10552791e-02 -1.55144110e-02 -5.17369136e-02  2.32332394e-01\n -2.81138662e-02 -5.28983735e-02  1.10283457e-02 -3.25165391e-02\n  6.33819550e-02  6.27487246e-03  1.34597858e-02  6.39182255e-02\n -4.25664382e-03 -3.08669750e-02 -1.06745608e-01  1.33701526e-02\n -2.14369670e-02 -1.20886136e-02  4.32616733e-02 -2.24008206e-02\n  2.39673350e-02  9.03356150e-02 -4.65076081e-02 -1.70566440e-02\n  7.07052946e-02 -1.08418139e-02 -2.38671023e-02  5.17465957e-02\n -6.05099611e-02  1.25991311e-02  6.42095655e-02 -3.63908513e-33\n  7.63550028e-02 -1.18246324e-01  2.63925344e-02  8.24378282e-02\n  9.85181183e-02  5.09256534e-02 -1.90523025e-02  1.38014052e-02\n -5.88014070e-03 -1.22107174e-02  1.84518863e-02  2.66982559e-02\n -3.47180329e-02 -7.05166087e-02  9.46039036e-02 -1.95524702e-03\n  4.53759916e-02  2.67891064e-02  6.37719929e-02  4.56029065e-02\n  1.73874795e-02  7.76933506e-02  1.42268864e-02  5.06489985e-02\n -6.87601138e-03  7.43265897e-02 -2.69637443e-02 -3.17649618e-02\n  2.00517345e-02  5.86697692e-03  2.00586561e-02 -2.57299072e-03\n -4.29737382e-02 -4.47464585e-02  6.34510582e-03  9.79925599e-03\n -5.31839654e-02  7.48026883e-03 -3.96348685e-02 -5.67922518e-02\n -2.95490175e-02  2.49238666e-02 -7.54031911e-02  1.52737554e-03\n  4.44060713e-02 -5.21348268e-02  1.19987927e-01  2.30146497e-02\n  2.76325922e-02  3.87760513e-02 -2.23546270e-02 -5.15341796e-02\n -6.65889531e-02 -5.44329882e-02 -3.16413282e-03 -9.78409685e-03\n -1.15129715e-02  4.79595587e-02 -3.84772010e-02 -3.95741165e-02\n  3.00681405e-02  3.73830274e-02 -2.41999719e-02 -2.46245526e-02\n  1.35157816e-02  2.43027066e-03  2.31515933e-02 -2.73636915e-02\n -3.65810357e-02  8.75698775e-03 -8.16444773e-03  2.35409383e-03\n  1.72999979e-03  3.72708850e-02  3.40095796e-02  5.71018609e-04\n  8.38144422e-02 -3.43044475e-02  2.35854015e-02  9.34631471e-03\n  6.47931695e-02  8.83999988e-02  1.69528760e-02 -4.98427115e-02\n  1.18394271e-01 -2.71005388e-02  1.47735700e-02 -1.47563949e-01\n -1.47719253e-02  6.30810158e-03 -1.52189821e-01  4.03645299e-02\n  5.02228849e-02 -5.23113050e-02 -5.12563847e-02  3.10715042e-33\n  3.70114818e-02 -6.94591505e-03  1.24944840e-02 -9.98814311e-03\n -8.73593092e-02 -3.71252559e-02 -6.14882894e-02  1.70788988e-01\n -3.28544155e-02  9.08840001e-02  9.51194465e-02 -7.04326760e-03\n  1.29705116e-01  4.23985766e-03  3.88105313e-04 -3.27869877e-03\n  1.04587317e-01 -3.60901728e-02 -1.75998791e-03  5.99894347e-03\n  1.06837079e-02 -1.98513046e-02 -8.24162290e-02 -8.27461258e-02\n -1.80127919e-02  1.81850493e-02  1.33806868e-02 -3.84799694e-03\n -5.46287186e-02 -2.22200397e-02  2.72562318e-02  7.37458188e-03\n -1.19097938e-03  4.21661250e-02 -6.84376583e-02  5.98912202e-02\n -1.13329805e-01  8.25650431e-03 -4.77013811e-02 -3.35780047e-02\n -5.79085052e-02 -6.47541508e-03  2.96426024e-02  1.46022672e-02\n -6.18912950e-02  1.96032207e-02 -8.95709470e-02  7.72806332e-02\n  4.99769375e-02  1.61434654e-02 -1.22335833e-02  2.23419983e-02\n  1.52471503e-02 -6.93452172e-03 -3.75061221e-02 -4.09000590e-02\n -4.33778763e-02 -1.27590373e-02  8.34930036e-03 -4.31568474e-02\n  3.96386087e-02 -2.43150946e-02  1.87552813e-02  7.43079558e-02\n -2.02280823e-02 -1.73118804e-02  2.55504418e-02  1.03637420e-01\n -4.71260995e-02 -5.40865287e-02 -4.01738919e-02  4.69997711e-02\n -1.10767402e-01 -1.70044024e-02 -3.35172657e-03  4.82833795e-02\n -4.26192097e-02 -9.93831269e-03 -2.42970623e-02  1.84097998e-02\n  2.25905962e-02  1.17199048e-02  2.48412136e-02  1.70990992e-02\n  6.63774982e-02 -4.30517867e-02 -4.59276373e-03  3.29935513e-02\n -7.99234584e-03  5.34392074e-02  8.04398768e-03 -5.45672961e-02\n -1.06293568e-02 -7.02302624e-03 -3.85855399e-02 -1.32938611e-08\n -5.12642078e-02  1.72419101e-02  3.68793383e-02 -6.42038584e-02\n -4.44006994e-02  1.33224728e-03  9.72847566e-02  4.35527600e-02\n -9.52245109e-03  8.40128288e-02  3.21813487e-02  4.00765017e-02\n -3.05083599e-02  7.04162195e-02 -6.42762706e-02  2.22297236e-02\n  3.89954634e-02 -5.15592247e-02  4.24882397e-02  6.44216016e-02\n -5.18949255e-02 -7.07421675e-02  1.74471512e-02 -4.15531266e-03\n  6.93204105e-02 -2.50402074e-02 -4.39300984e-02 -4.08832878e-02\n  3.90965343e-02 -9.12383199e-02  4.56751734e-02  2.09934060e-02\n -6.13956433e-03  1.06612397e-02 -1.03664458e-01 -3.49144302e-02\n -1.64234638e-02  7.88927600e-02 -1.32319219e-02  2.76354086e-02\n -4.23146784e-02  1.32851437e-01 -3.24702635e-02 -4.87504788e-02\n  2.72558816e-02 -3.24229673e-02  1.28434300e-01  5.26647922e-03\n -5.10121249e-02 -1.73536912e-02 -1.83033925e-02 -2.82469261e-02\n -6.07104693e-03 -3.85981314e-02  8.50153491e-02 -1.20294048e-02\n -2.18695942e-02  7.69673288e-02 -6.12484850e-02  1.38297966e-02\n  9.33432207e-02 -1.15287835e-02 -1.81885641e-02 -4.10755463e-02]"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello world</td>\n",
       "      <td>[-0.03447723, 0.031023242, 0.006735021, 0.0261...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goodbye world</td>\n",
       "      <td>[0.02274555, 0.08009697, 0.033089258, 0.001067...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text                                             vector\n",
       "0    hello world  [-0.03447723, 0.031023242, 0.006735021, 0.0261...\n",
       "1  goodbye world  [0.02274555, 0.08009697, 0.033089258, 0.001067..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that we've created our first table in LanceDB! Now we'll walk through how to do full text search with our table. This is a simple method which provides a strong baseline for our retrieval system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Text Search\n",
    "\n",
    "> **Important** : Before running this code, make sure you've installed `tantivy==0.20.1` in your local kernel. This is important because LanceDB uses Tantivy under the hood to perform FTS.\n",
    "\n",
    "By default, LanceDB uses vector search to perform any query, This means that when we query our table, it'll use the vector embeddings to find the most similar documents to our query.\n",
    "\n",
    "In order to use Full Text Search instead, we need to explicitly set the query type to `fts` in order for it to work. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'hello world',\n",
       "  'vector': [-0.03447723016142845,\n",
       "   0.031023241579532623,\n",
       "   0.006735020782798529,\n",
       "   0.026109028607606888,\n",
       "   -0.039362046867609024,\n",
       "   -0.16030247509479523,\n",
       "   0.06692402064800262,\n",
       "   -0.006441468372941017,\n",
       "   -0.04745054990053177,\n",
       "   0.01475888304412365,\n",
       "   0.07087527960538864,\n",
       "   0.05552754923701286,\n",
       "   0.019193338230252266,\n",
       "   -0.026251371949911118,\n",
       "   -0.010109517723321915,\n",
       "   -0.02694053389132023,\n",
       "   0.022307438775897026,\n",
       "   -0.022226599976420403,\n",
       "   -0.1496926248073578,\n",
       "   -0.017493126913905144,\n",
       "   0.007676322478801012,\n",
       "   0.054352279752492905,\n",
       "   0.003254470182582736,\n",
       "   0.03172598034143448,\n",
       "   -0.08462142199277878,\n",
       "   -0.02940603718161583,\n",
       "   0.05159568414092064,\n",
       "   0.048124056309461594,\n",
       "   -0.0033148089423775673,\n",
       "   -0.058279216289520264,\n",
       "   0.041969288140535355,\n",
       "   0.022210724651813507,\n",
       "   0.1281888484954834,\n",
       "   -0.022338956594467163,\n",
       "   -0.011656180024147034,\n",
       "   0.06292832642793655,\n",
       "   -0.03287629038095474,\n",
       "   -0.09122608602046967,\n",
       "   -0.03117540292441845,\n",
       "   0.052699580788612366,\n",
       "   0.0470348559319973,\n",
       "   -0.08420306444168091,\n",
       "   -0.03005615621805191,\n",
       "   -0.02074478752911091,\n",
       "   0.009517795406281948,\n",
       "   -0.0037217361386865377,\n",
       "   0.007343305740505457,\n",
       "   0.039324261248111725,\n",
       "   0.0932740867137909,\n",
       "   -0.003788589732721448,\n",
       "   -0.05274214968085289,\n",
       "   -0.05805812403559685,\n",
       "   -0.0068644010461866856,\n",
       "   0.0052832355722785,\n",
       "   0.08289297670125961,\n",
       "   0.019362738355994225,\n",
       "   0.006284520961344242,\n",
       "   -0.010330811142921448,\n",
       "   0.00903236959129572,\n",
       "   -0.03768380731344223,\n",
       "   -0.04520609974861145,\n",
       "   0.024016346782445908,\n",
       "   -0.006944183260202408,\n",
       "   0.013491624034941196,\n",
       "   0.10005494952201843,\n",
       "   -0.0716838464140892,\n",
       "   -0.021695060655474663,\n",
       "   0.03161846473813057,\n",
       "   -0.051634613424539566,\n",
       "   -0.08224766701459885,\n",
       "   -0.06569328159093857,\n",
       "   -0.009895392693579197,\n",
       "   0.005816400516778231,\n",
       "   0.07355451583862305,\n",
       "   -0.034050311893224716,\n",
       "   0.0248861126601696,\n",
       "   0.01448807306587696,\n",
       "   0.026457404717803,\n",
       "   0.009656744077801704,\n",
       "   0.030217330902814865,\n",
       "   0.052803970873355865,\n",
       "   -0.07535989582538605,\n",
       "   0.009897199459373951,\n",
       "   0.029836812987923622,\n",
       "   0.01755557768046856,\n",
       "   0.023092016577720642,\n",
       "   0.0019338455749675632,\n",
       "   0.0014001853996887803,\n",
       "   -0.047175969928503036,\n",
       "   -0.011194366030395031,\n",
       "   -0.11420139670372009,\n",
       "   -0.019812000915408134,\n",
       "   0.04026619717478752,\n",
       "   0.0021929836366325617,\n",
       "   -0.07979221642017365,\n",
       "   -0.025382323190569878,\n",
       "   0.09448292851448059,\n",
       "   -0.02898116409778595,\n",
       "   -0.14500252902507782,\n",
       "   0.23097743093967438,\n",
       "   0.02773115411400795,\n",
       "   0.03211146593093872,\n",
       "   0.03106500953435898,\n",
       "   0.04283282160758972,\n",
       "   0.06423782557249069,\n",
       "   0.03216322138905525,\n",
       "   -0.004876680206507444,\n",
       "   0.05569946765899658,\n",
       "   -0.03753237798810005,\n",
       "   -0.021505627781152725,\n",
       "   -0.028342682868242264,\n",
       "   -0.028846928849816322,\n",
       "   0.0383530855178833,\n",
       "   -0.017468692734837532,\n",
       "   0.052485305815935135,\n",
       "   -0.07487601786851883,\n",
       "   -0.03125980496406555,\n",
       "   0.02184153161942959,\n",
       "   -0.03989570587873459,\n",
       "   -0.00858711265027523,\n",
       "   0.02695661410689354,\n",
       "   -0.04849547892808914,\n",
       "   0.011469840072095394,\n",
       "   0.02961825020611286,\n",
       "   -0.020572194829583168,\n",
       "   0.013103912584483624,\n",
       "   0.02883339859545231,\n",
       "   -3.1941990848222185e-33,\n",
       "   0.06478200852870941,\n",
       "   -0.01813018135726452,\n",
       "   0.05178993195295334,\n",
       "   0.12198270857334137,\n",
       "   0.028780203312635422,\n",
       "   0.008721974678337574,\n",
       "   -0.07052119821310043,\n",
       "   -0.016907254233956337,\n",
       "   0.04073972627520561,\n",
       "   0.0421161949634552,\n",
       "   0.02544722892343998,\n",
       "   0.03574622794985771,\n",
       "   -0.049144770950078964,\n",
       "   0.0021290958393365145,\n",
       "   -0.015546589158475399,\n",
       "   0.050730619579553604,\n",
       "   -0.04818529635667801,\n",
       "   0.03588061407208443,\n",
       "   -0.004067062865942717,\n",
       "   0.10172473639249802,\n",
       "   -0.05597001314163208,\n",
       "   -0.010680998675525188,\n",
       "   0.011235842481255531,\n",
       "   0.09068653732538223,\n",
       "   0.004234465304762125,\n",
       "   0.03513862565159798,\n",
       "   -0.00970288272947073,\n",
       "   -0.09386522322893143,\n",
       "   0.0928555428981781,\n",
       "   0.008004955016076565,\n",
       "   -0.007705391850322485,\n",
       "   -0.05208674445748329,\n",
       "   -0.01258796826004982,\n",
       "   0.0032668791245669127,\n",
       "   0.006013552658259869,\n",
       "   0.007581565994769335,\n",
       "   0.010517135262489319,\n",
       "   -0.08634551614522934,\n",
       "   -0.06987874209880829,\n",
       "   -0.0025338532868772745,\n",
       "   -0.09097656607627869,\n",
       "   0.04688732698559761,\n",
       "   0.05207650735974312,\n",
       "   0.007193928584456444,\n",
       "   0.010903569869697094,\n",
       "   -0.005229488480836153,\n",
       "   0.013937355019152164,\n",
       "   0.021968308836221695,\n",
       "   0.034208547323942184,\n",
       "   0.060224682092666626,\n",
       "   0.00011663387704174966,\n",
       "   0.014731933362782001,\n",
       "   -0.07008922100067139,\n",
       "   0.028499027714133263,\n",
       "   -0.027601614594459534,\n",
       "   0.010768385604023933,\n",
       "   0.034830961376428604,\n",
       "   -0.022487889975309372,\n",
       "   0.009769082069396973,\n",
       "   0.07722780853509903,\n",
       "   0.02158835157752037,\n",
       "   0.11495619267225266,\n",
       "   -0.06800111383199692,\n",
       "   0.023760925978422165,\n",
       "   -0.01598392240703106,\n",
       "   -0.017826974391937256,\n",
       "   0.06439494341611862,\n",
       "   0.032025713473558426,\n",
       "   0.050270259380340576,\n",
       "   -0.005913660395890474,\n",
       "   -0.03370801359415054,\n",
       "   0.017840377986431122,\n",
       "   0.01657339744269848,\n",
       "   0.06329658627510071,\n",
       "   0.03467719256877899,\n",
       "   0.04647345095872879,\n",
       "   0.09790613502264023,\n",
       "   -0.006635486148297787,\n",
       "   0.02520706132054329,\n",
       "   -0.0779883936047554,\n",
       "   0.016926443204283714,\n",
       "   -0.0009458480635657907,\n",
       "   0.022471925243735313,\n",
       "   -0.038253188133239746,\n",
       "   0.09570477902889252,\n",
       "   -0.005350769497454166,\n",
       "   0.010469023138284683,\n",
       "   -0.11524050682783127,\n",
       "   -0.013262532651424408,\n",
       "   -0.01070939376950264,\n",
       "   -0.08311722427606583,\n",
       "   0.07327356934547424,\n",
       "   0.04939219355583191,\n",
       "   -0.00899438001215458,\n",
       "   -0.09584559500217438,\n",
       "   3.36614782706661e-33,\n",
       "   0.12493178993463516,\n",
       "   0.019349707290530205,\n",
       "   -0.05822576582431793,\n",
       "   -0.03598829358816147,\n",
       "   -0.05074677988886833,\n",
       "   -0.04566233977675438,\n",
       "   -0.08260347694158554,\n",
       "   0.148194819688797,\n",
       "   -0.08842112123966217,\n",
       "   0.06027441844344139,\n",
       "   0.051030274480581284,\n",
       "   0.010303126648068428,\n",
       "   0.1412142515182495,\n",
       "   0.03081381879746914,\n",
       "   0.06103309988975525,\n",
       "   -0.05285123363137245,\n",
       "   0.13664884865283966,\n",
       "   0.009189914911985397,\n",
       "   -0.017325304448604584,\n",
       "   -0.012848644517362118,\n",
       "   -0.007995270192623138,\n",
       "   -0.05098011717200279,\n",
       "   -0.052350640296936035,\n",
       "   0.007593029178678989,\n",
       "   -0.01516625378280878,\n",
       "   0.016960347071290016,\n",
       "   0.021270517259836197,\n",
       "   0.020557977259159088,\n",
       "   -0.12002800405025482,\n",
       "   0.014461789280176163,\n",
       "   0.026759885251522064,\n",
       "   0.025330550968647003,\n",
       "   -0.0427546501159668,\n",
       "   0.0067684231325984,\n",
       "   -0.01445857249200344,\n",
       "   0.04526206851005554,\n",
       "   -0.09147656708955765,\n",
       "   -0.019439175724983215,\n",
       "   -0.01783348247408867,\n",
       "   -0.05491010844707489,\n",
       "   -0.05264109745621681,\n",
       "   -0.010459030978381634,\n",
       "   -0.05201601982116699,\n",
       "   0.020892120897769928,\n",
       "   -0.07997029274702072,\n",
       "   -0.01211128756403923,\n",
       "   -0.05773139372467995,\n",
       "   0.023178284987807274,\n",
       "   -0.008031673729419708,\n",
       "   -0.02598925493657589,\n",
       "   -0.07995667308568954,\n",
       "   -0.020728839561343193,\n",
       "   0.048817671835422516,\n",
       "   -0.020389167591929436,\n",
       "   -0.049176640808582306,\n",
       "   0.014159671030938625,\n",
       "   -0.06362210959196091,\n",
       "   -0.007807434070855379,\n",
       "   0.016431540250778198,\n",
       "   -0.025682566687464714,\n",
       "   0.013381082564592361,\n",
       "   0.026248762384057045,\n",
       "   0.009978348389267921,\n",
       "   0.06322894245386124,\n",
       "   0.0026721099857240915,\n",
       "   -0.00658275606110692,\n",
       "   0.016632014885544777,\n",
       "   0.03236643970012665,\n",
       "   0.03794253244996071,\n",
       "   -0.036376044154167175,\n",
       "   -0.006910949479788542,\n",
       "   0.0001596329384483397,\n",
       "   -0.001633552135899663,\n",
       "   -0.02727823704481125,\n",
       "   -0.028038037940859795,\n",
       "   0.04968155175447464,\n",
       "   -0.02886718139052391,\n",
       "   -0.002418110379949212,\n",
       "   0.014774897135794163,\n",
       "   0.009764536283910275,\n",
       "   0.005797558464109898,\n",
       "   0.01348612830042839,\n",
       "   0.0055679320357739925,\n",
       "   0.03722710162401199,\n",
       "   0.007232516538351774,\n",
       "   0.04015625640749931,\n",
       "   0.08150319755077362,\n",
       "   0.07199163734912872,\n",
       "   -0.013056159019470215,\n",
       "   -0.042882055044174194,\n",
       "   -0.011011221446096897,\n",
       "   0.004897822625935078,\n",
       "   -0.009229760617017746,\n",
       "   0.03519144654273987,\n",
       "   -0.051034994423389435,\n",
       "   -1.571437557856825e-08,\n",
       "   -0.08862442523241043,\n",
       "   0.02390930987894535,\n",
       "   -0.016238732263445854,\n",
       "   0.03170041739940643,\n",
       "   0.0272841677069664,\n",
       "   0.05246879532933235,\n",
       "   -0.047070953994989395,\n",
       "   -0.058847468346357346,\n",
       "   -0.0632082149386406,\n",
       "   0.04088855907320976,\n",
       "   0.049827929586172104,\n",
       "   0.10655169934034348,\n",
       "   -0.07450231909751892,\n",
       "   -0.012495465576648712,\n",
       "   0.0183706097304821,\n",
       "   0.03947417065501213,\n",
       "   -0.02479786053299904,\n",
       "   0.014516300521790981,\n",
       "   -0.03706922009587288,\n",
       "   0.020015714690089226,\n",
       "   -4.862520290771499e-05,\n",
       "   0.009866534732282162,\n",
       "   0.024838745594024658,\n",
       "   -0.052458059042692184,\n",
       "   0.029314104467630386,\n",
       "   -0.08719195425510406,\n",
       "   -0.014499744400382042,\n",
       "   0.026019077748060226,\n",
       "   -0.01874641887843609,\n",
       "   -0.07620516419410706,\n",
       "   0.035043273121118546,\n",
       "   0.10363949835300446,\n",
       "   -0.028050534427165985,\n",
       "   0.012718254700303078,\n",
       "   -0.07632549852132797,\n",
       "   -0.018652409315109253,\n",
       "   0.024976687505841255,\n",
       "   0.0814453512430191,\n",
       "   0.06875889748334885,\n",
       "   -0.06405661255121231,\n",
       "   -0.08389393240213394,\n",
       "   0.06136239320039749,\n",
       "   -0.03354557976126671,\n",
       "   -0.10615336894989014,\n",
       "   -0.04008054733276367,\n",
       "   0.032530225813388824,\n",
       "   0.07662487775087357,\n",
       "   -0.07301614433526993,\n",
       "   0.00033759165671654046,\n",
       "   -0.040871571749448776,\n",
       "   -0.07578843086957932,\n",
       "   0.02752765081822872,\n",
       "   0.0746254250407219,\n",
       "   0.01771729812026024,\n",
       "   0.09121835231781006,\n",
       "   0.11022016406059265,\n",
       "   0.0005698329186998308,\n",
       "   0.05146333575248718,\n",
       "   -0.014551276341080666,\n",
       "   0.03323192894458771,\n",
       "   0.023792274296283722,\n",
       "   -0.02288978174328804,\n",
       "   0.03893756866455078,\n",
       "   0.030206838622689247],\n",
       "  '_score': 0.6931471824645996}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.search(\"hello\", query_type=\"fts\").to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when we run the code above, we get the error that an inverted index is not found. This is a valid error because in order for us to perform full text search, we need to create an index which maps keywords to the documents that contain them.\n",
    "\n",
    "### What is an Inverted Index?\n",
    "\n",
    "An inverted index is a data structure that allows us to quickly look up which documents contain a given keyword. It's a key component of full text search systems and is used to speed up the search process.\n",
    "\n",
    "it maps words or subwords to the documents that contain them. We need to generate this ahead of time so that we can perform full text search efficiently. \n",
    "\n",
    "Let's see a simplified example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have an initial document\n",
    "documents = {\n",
    "    1: \"The quick brown fox\",\n",
    "    2: \"The lazy brown dog\",\n",
    "    3: \"The fox jumps over dog\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might then do some pre-processing here to break down the contents of each documents into subwords or words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {\n",
    "    \"the\": {1, 2, 3},\n",
    "    \"quick\": {1},\n",
    "    \"brown\": {1, 2},\n",
    "    \"fox\": {1, 3},\n",
    "    \"lazy\": {2},\n",
    "    \"dog\": {2, 3},\n",
    "    \"jumps\": {3},\n",
    "    \"over\": {3},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that when users make a query like `the dog`, we can quickly look up the documents that contain these words and return the results. \n",
    "\n",
    "We use a simplified implementation here we check for each word in the query and then return the documents that contain any of the words. A document that has more matches has a higher score and will be ranked higher in the returned results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results: [{'doc_id': 1, 'score': 1.0}, {'doc_id': 2, 'score': 0.5}, {'doc_id': 3, 'score': 0.5}]\n"
     ]
    }
   ],
   "source": [
    "def search(query, inverted_index):\n",
    "    # Convert query to lowercase and split into words\n",
    "    query_words = query.lower().split()\n",
    "\n",
    "    # Count matches for each document\n",
    "    doc_scores = {}\n",
    "    for word in query_words:\n",
    "        if word in inverted_index:\n",
    "            for doc_id in inverted_index[word]:\n",
    "                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + 1\n",
    "\n",
    "    # Sort documents by score in descending order\n",
    "    sorted_results = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Return list of (doc_id, score) tuples\n",
    "    return [\n",
    "        {\"doc_id\": doc_id, \"score\": score / len(query_words)}\n",
    "        for doc_id, score in sorted_results\n",
    "    ]\n",
    "\n",
    "\n",
    "query = \"the quick\"\n",
    "results = search(query, inverted_index)\n",
    "print(f\"Search results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LanceDB automates this process for us and makes it incredibly easy to perform full text search with the `create_fts_index` method. A few things are different here, they use a scoring mechanism called `BM25` and more pre-processing is done for both your queries and documents themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.create_fts_index(\"text\", replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that our full text search works nicely out of the box now. Additionally with Tantivy, we can use boolean queries to combine multiple words as seen below where we combine `hello` and `goodbye` in our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "for item in table.search(\"hello\", query_type=\"fts\").to_list():\n",
    "    print(item[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goodbye world\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "for item in table.search(\"hello OR goodbye\", query_type=\"fts\").to_list():\n",
    "    print(item[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Different Retrieval Methods\n",
    "\n",
    "Let's quickly review the different retrieval methods that we've seen so far.\n",
    "\n",
    "1. **Vector Search** : Vector Search converts text into number sequences (vectors) that capture meaning. When you search, it finds documents whose vectors are closest to your query vector. This works well for finding semantically similar content, even if the exact words don't match. For example, \"I'm delighted\" and \"I'm really happy\" would be considered similar.\n",
    "2. **Full text Search** : Full Text Search directly matches the words in your query to words in documents. It uses techniques like BM25 scoring, which ranks documents based on how often your search terms appear and how unique those terms are across all documents. This is great for finding exact matches or specific keywords.\n",
    "3. **Hybrid Search** : Hybrid Search combines both approaches to get the best of both worlds. It can find documents that either contain your exact keywords or express similar meanings. We make a query using both search results and then combine the final set of retrieved results in a given way to return a new set of results.\n",
    "\n",
    "Let's see how we can perform hybrid search in LanceDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "goodbye world\n"
     ]
    }
   ],
   "source": [
    "for item in table.search(\"I'm really excited!\", query_type=\"hybrid\").to_list():\n",
    "    print(item[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the results are semantically similar to our query. Hello World is much closer to say \"I'm really excited!\" than Goodbye World. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Re-Rankers\n",
    "\n",
    "While basic search methods work well for simple queries, more complex questions often benefit from re-ranking. \n",
    "\n",
    "A re-ranker takes an initial set of search results and applies a more sophisticated model to analyze how well each result actually answers your query. \n",
    "\n",
    "For example, in our capital punishment example below, hybrid search returns documents that contain the relevant keywords. But the re-ranker can better understand we're asking about location rather than general information, helping it prioritize the Washington D.C. result. This additional analysis takes more computation time but often produces significantly better results for complex queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's create some more complex documents\n",
    "documents = [\n",
    "    \"Carson City is the capital city of the American state of Nevada.\",\n",
    "    \"The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.\",\n",
    "    \"Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district. \",\n",
    "    \"Capital punishment (the death penalty) has existed in the United States since before the United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states.\",\n",
    "]\n",
    "documents = [{\"text\": doc} for doc in documents]\n",
    "\n",
    "# Create a new table for our complex documents\n",
    "complex_table = db.create_table(\"complex_docs\", data=documents, schema=Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index for full text search\n",
    "complex_table.create_fts_index([\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Capital punishment (the death penalty) has existed in the United States since before the United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states.\n",
      "\n",
      "2. Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district. \n",
      "\n",
      "3. Carson City is the capital city of the American state of Nevada.\n",
      "\n",
      "4. The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.\n"
     ]
    }
   ],
   "source": [
    "# Let's try a search query and see the results before reranking\n",
    "query = \"where did the capital of the United States decide to allow capital punishment?\"\n",
    "\n",
    "results = complex_table.search(query, query_type=\"hybrid\").limit(4).to_list()\n",
    "for idx, item in enumerate(results):\n",
    "    print(f\"\\n{idx + 1}. {item['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good start but we can see that the results are not exactly what we want. We're asking for a location where the capital punishment decision was made instead of when capital punishment was allowed.\n",
    "\n",
    "In this case we can solve this by using a `ReRanker`. Re-rankers are more expensive than just doing simple vector search but they often allow us to improve the results of our retrieval system by a large margin.\n",
    "\n",
    "We can use the `rerank` method in LanceDB to re-rank our results. We'll use the `CohereReranker` to do so. We'll also use the `rerank_top_k` parameter to limit the number of results that we return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ColBERTRanker model colbert-ir/colbertv2.0 (this message can be suppressed by setting verbose=0)\n",
      "No device set\n",
      "Using device cpu\n",
      "No dtype set\n",
      "Using dtype torch.float32\n",
      "Loading model colbert-ir/colbertv2.0, this might take a while...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97474c4508304a22bf8bc71f4cac0f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ee85e057bd4c02afa48ce23616706f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26ecfe6b686475ca1682ace210d71d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66fb3deac42433791d4c33a926d40bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22e0e2986f04e17882fdce9f5158959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f949dc445fb4779991af9069c219c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Dim set to: 128 for downcasting\n",
      "\n",
      "1. Capital punishment (the death penalty) has existed in the United States since before the United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states.\n",
      "\n",
      "2. Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district. \n",
      "\n",
      "3. Carson City is the capital city of the American state of Nevada.\n",
      "\n",
      "4. The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.\n"
     ]
    }
   ],
   "source": [
    "from lancedb.rerankers import ColbertReranker\n",
    "\n",
    "# Let's try a search query and see the results before reranking\n",
    "query = \"where did the capital of the United States decide to allow capital punishment?\"\n",
    "\n",
    "# Then Define a Cohere Reranker\n",
    "reranker = ColbertReranker()\n",
    "\n",
    "results = (\n",
    "    complex_table.search(query, query_type=\"hybrid\").rerank(reranker).limit(4).to_list()\n",
    ")\n",
    "for idx, item in enumerate(results):\n",
    "    print(f\"\\n{idx + 1}. {item['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored the core features that make LanceDB well-suited for RAG applications. We learned how to:\n",
    "\n",
    "1. Define clean schemas using Pydantic models\n",
    "2. Index and search data using different methods\n",
    "3. Leverage the Re-Ranking API to improve the results of our retrieval system\n",
    "\n",
    "LanceDB's combination of type safety through Pydantic, automated embedding handling, and unified search API provides a strong foundation for RAG development. These capabilities will be especially valuable in future weeks as we evaluate and improve different aspects of our retrieval system. Understanding these LanceDB basics ensures you can follow along with the rest of the course and make the most out of it.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
