{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 : Systematically Improving Your RAG Application\n",
    "\n",
    "## Benchmarking Retrieval Methods\n",
    "\n",
    "> If you have not already, please run the `1. synthetic_questions.ipynb` notebook to generate the synthetic questions we'll be using in this notebook before proceeding.\n",
    "\n",
    "After generating the synthetic questions, we'll use them to benchmark the recall and mrr of various retrievel methods. We want to do so because these metrics give us an objective measure of how well a retrieval method is performing.\n",
    "\n",
    "This allows us to make an informed decision about whether a retrieval method might be worth the cost and latency to implement. For instance, if we get a 1% improvement in recall@10 but latency increases by 10x, we might not want to use that method.\n",
    "\n",
    "We'll do this in two steps\n",
    "\n",
    "1. First, we'll take our original dataset of sql snippets from `567-labs/bird-rag` and ingest it into a local lancedb instance.\n",
    "2. Then we'll show you how to measure recall and mrr at different levels of k\n",
    "\n",
    "Recall that in our original dataset, each row had the following columns\n",
    "\n",
    "- `id`: The unique identifier for each SQL snippet\n",
    "- `query`: The SQL snippet\n",
    "- `difficulty`: The difficulty of the question\n",
    "\n",
    "When we query our database with a synthetic question, we'll retrieve a list of sql snippets and their ids that match the query. Our goal here is to verify that we're able to retrieve the correct id for each question. \n",
    "\n",
    "We'll be using `braintrust` to collect the data and discuss the trade offs of each method. We like `braintrust` because it allows us to easily run multiple experiments using a simple `Eval` object and share the results easily if you're working with a team.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Our Rag Pipeline\n",
    "\n",
    "In this example, we're using a local `lancedb` instance. We're doing so because of 3 reasons.\n",
    "\n",
    "1. LanceDB handles the embeddings of our data for us \n",
    "2. It provides embedding search, hybrid search and other re-ranking methods all within a single api.\n",
    "3. We can use Pydantic to define our table schema and easily ingest our data.\n",
    "\n",
    "This makes it quick and easy for us to compare the performance of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# import lancedb\n",
    "# from lancedb.pydantic import LanceModel, Vector\n",
    "# from lancedb.embeddings import get_registry\n",
    "\n",
    "# # Create LanceDB Instance\n",
    "# db = lancedb.connect(\"./lancedb\")\n",
    "\n",
    "# # Define and create Table using Pydantic\n",
    "# func = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "# class Chunk(LanceModel):\n",
    "#     id: str\n",
    "#     query: str = func.SourceField()\n",
    "#     vector: Vector(func.ndims()) = func.VectorField()\n",
    "\n",
    "\n",
    "# table = db.create_table(\"chunks\", schema=Chunk, mode=\"overwrite\")\n",
    "\n",
    "# # Ingest dataset into table\n",
    "# dataset = datasets.load_dataset(\"567-labs/bird-rag\")[\"train\"]\n",
    "# formatted_dataset = [{\"id\": item[\"id\"], \"query\": item[\"query\"]} for item in dataset]\n",
    "\n",
    "# table.add(formatted_dataset)\n",
    "\n",
    "# table.create_fts_index(\"query\", replace=True)\n",
    "# print(f\"{table.count_rows()} chunks ingested into the database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "table = db.open_table(\"chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Metrics\n",
    "\n",
    "Let's now start by evaluating the retrieval performance of our model. We'll do so by measuring the recall and MRR at different levels of k.\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{Number of Relevant Items Retrieved}}{\\text{Total Number of Relevant Items}} $$ \n",
    "\n",
    "$$ \\text{MRR} = \\frac{\\sum_{i=1}^{n} \\frac{1}{rank(i)}}{n} $$ \n",
    "\n",
    "As models improve, their context window and reasoning abilities improve. This means that their ability to select relevant information in response to a user query will improve. By optimizing for recall, we ensure that the language model has access to all necessary information, which can lead to more accurate and reliable generated responses.\n",
    "\n",
    "MRR@K is a useful metric if we want to display retrieved results as citations to users. We normally show a smaller list of retrieved results to users and we want to make sure that the correct result is ranked highly during retrieval so that it's more likely to be selected.\n",
    "\n",
    "<TODO: Jason please verify or add on here >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(predictions: list[str], gt: list[str]):\n",
    "    mrr = 0\n",
    "    for label in gt:\n",
    "        if label in predictions:\n",
    "            # Find the relevant item that has the smallest index\n",
    "            mrr = max(mrr, 1 / (predictions.index(label) + 1))\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def calculate_recall(predictions: list[str], gt: list[str]):\n",
    "    # Calculate the proportion of relevant items that were retrieved\n",
    "    return len([label for label in gt if label in predictions]) / len(gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Metrics\n",
    "\n",
    "We want to determine whether we can retrieve the relevant SQL snippet for each synthetic question. Remember that in our earlier notebook, we generated the question below for the corresponding SQL snippet.\n",
    "\n",
    "Question : Which schools in France are locally funded and have a greater difference between their total K-12 \n",
    "enrollment and enrollment for ages 5-17 compared to the average difference for locally funded schools?\n",
    "\n",
    "Snippet \n",
    "```sql\n",
    "SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE\n",
    "T2.FundingType = 'Locally funded' AND (T1.`Enrollment (K-12)` - T1.`Enrollment (Ages 5-17)`) > (SELECT \n",
    "AVG(T3.`Enrollment (K-12)` - T3.`Enrollment (Ages 5-17)`) FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
    "T4.CDSCode WHERE T4.FundingType = 'Locally funded')\n",
    "```\n",
    "\n",
    "Let's see this in action by fetching the top 25 results from our database that match this query using embedding search. Notice here that we're using the chunk id to calculate recall and mrr.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span> SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE \n",
       "T2.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span> AND <span style=\"font-weight: bold\">(</span>T1.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T1.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> &gt; <span style=\"font-weight: bold\">(</span>SELECT \n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AVG</span><span style=\"font-weight: bold\">(</span>T3.`Enrollment <span style=\"font-weight: bold\">(</span>K-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"font-weight: bold\">)</span>` - T3.`Enrollment <span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>`<span style=\"font-weight: bold\">)</span> FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = <span style=\"color: #008000; text-decoration-color: #008000\">'Locally funded'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m28\u001b[0m SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE \n",
       "T2.FundingType = \u001b[32m'Locally funded'\u001b[0m AND \u001b[1m(\u001b[0mT1.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T1.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m > \u001b[1m(\u001b[0mSELECT \n",
       "\u001b[1;35mAVG\u001b[0m\u001b[1m(\u001b[0mT3.`Enrollment \u001b[1m(\u001b[0mK-\u001b[1;36m12\u001b[0m\u001b[1m)\u001b[0m` - T3.`Enrollment \u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m`\u001b[1m)\u001b[0m FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = \n",
       "T4.CDSCode WHERE T4.FundingType = \u001b[32m'Locally funded'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span> SELECT T1.NCESSchool FROM schools AS T1 INNER JOIN frpm AS T2 ON T1.CDSCode = T2.CDSCode ORDER BY T2.`Enrollment\n",
       "<span style=\"font-weight: bold\">(</span>Ages <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"font-weight: bold\">)</span>` DESC LIMIT <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m14\u001b[0m SELECT T1.NCESSchool FROM schools AS T1 INNER JOIN frpm AS T2 ON T1.CDSCode = T2.CDSCode ORDER BY T2.`Enrollment\n",
       "\u001b[1m(\u001b[0mAges \u001b[1;36m5\u001b[0m-\u001b[1;36m17\u001b[0m\u001b[1m)\u001b[0m` DESC LIMIT \u001b[1;36m5\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "\n",
    "question = \"Which schools in France are locally funded and have a greater difference between their total K-12 enrollment and enrollment for ages 5-17 compared to the average difference for locally funded schools?\"\n",
    "\n",
    "query = \"SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.FundingType = 'Locally funded' AND (T1.`Enrollment (K-12)` - T1.`Enrollment (Ages 5-17)`) > (SELECT AVG(T3.`Enrollment (K-12)` - T3.`Enrollment (Ages 5-17)`) FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = T4.CDSCode WHERE T4.FundingType = 'Locally funded')\"\n",
    "\n",
    "retrieved_items = table.search(question).limit(25).to_list()\n",
    "\n",
    "for item in retrieved_items[:2]:\n",
    "    print(item[\"id\"], item[\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first item is a perfect match for our desired chunk. We're going to calculate the recall@25 and MRR@25 for this specific snippet. To do so, we'll use its id. The id of this specific snippet is 28. You can verify it by looking at the filtered subset of the hugging face dataset [here](https://huggingface.co/datasets/567-labs/bird-rag/viewer/default/train?q=T4.FundingType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">MRR@<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "MRR@\u001b[1;36m25\u001b[0m: \u001b[1;36m1.0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Recall@<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Recall@\u001b[1;36m25\u001b[0m: \u001b[1;36m1.0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Relevant item found at rank: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Relevant item found at rank: \u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_ids = [item[\"id\"] for item in retrieved_items]\n",
    "ground_truth_ids = [\"28\"]  # The ID of the expected SQL snippet\n",
    "\n",
    "# Calculate metrics\n",
    "mrr_score = calculate_mrr(predicted_ids, ground_truth_ids)\n",
    "recall_score = calculate_recall(predicted_ids, ground_truth_ids)\n",
    "rank_position = predicted_ids.index(\"28\") + 1  # Adding 1 because index starts at 0\n",
    "\n",
    "print(f\"MRR@25: {mrr_score}\")\n",
    "print(f\"Recall@25: {recall_score}\")\n",
    "print(f\"Relevant item found at rank: {rank_position}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple query since we have many of the specific column values in the snippet listed in the question - Eg. (K-12) and (Ages 5-17). But we're able to see how well our retrieval system is performing in this case with an objective measure.\n",
    "\n",
    "We use a wrapper to compute metrics for multiple subsets of the retrieved items without having to hard code functions that take in a subset of the retrieved items. This makes it easy to vary the size of k and the metrics we want to use instead of hard coding them.\n",
    "\n",
    "<TODO: Jason please verify this>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the names for our metrics\n",
    "eval_metrics = [[\"mrr\", calculate_mrr], [\"recall\", calculate_recall]]\n",
    "\n",
    "# Define the different sizes of k that we want to compute the metrics at\n",
    "sizes = [1, 3, 5, 10, 15, 25]\n",
    "\n",
    "# Define wrapper functions that will take a subset of k items and calculate the relevant metric\n",
    "metrics = {}\n",
    "for metric_name, metric_fn in eval_metrics:\n",
    "    for size in sizes:\n",
    "        key = f\"{metric_name}@{size}\"\n",
    "        metrics[key] = lambda predictions, gt, m=metric_fn, s=size: m(\n",
    "            predictions[:s], gt\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this in action below with a simple example.\n",
    "\n",
    "Our MRR@1 and Recall@1 will be 0 since there are no relevant items at k = 1. However, at k=3, we see that we have `a` in the top 3 retrieved items. This gives us a MRR@3 of 1/3 and a Recall@3 of 1/2 ( since we retrieved 1 of 2 relevant items).\n",
    "\n",
    "Once we look at the MRR@5, we see that we have `a` and `b` in the top 5 retrieved items. This gives us a MRR@5 of 1/3 and a Recall@5 of 2/2 (since we retrieved all of the relevant items). We can see that we have the same values for MRR@10 and Recall@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mrr@1': 0,\n",
       " 'mrr@3': 0.3333333333333333,\n",
       " 'mrr@5': 0.3333333333333333,\n",
       " 'mrr@10': 0.3333333333333333,\n",
       " 'recall@1': 0.0,\n",
       " 'recall@3': 0.5,\n",
       " 'recall@5': 1.0,\n",
       " 'recall@10': 1.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\"a\", \"b\"]\n",
    "preds = [\"x\", \"y\", \"a\", \"c\", \"b\", \"f\", \"g\", \"h\", \"i\", \"k\"]\n",
    "\n",
    "{metric: score_fn(preds, labels) for metric, score_fn in metrics.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running your first Benchmark\n",
    "\n",
    "We use braintrust here because it's easy to setup complex experiments. We can use the `Eval` object to define a single experiment and run it with different configurations. \n",
    "\n",
    "We can do so by defining a \n",
    "\n",
    "- `Task` : This is a function that takes in the input and returns an expected output\n",
    "- `Scorer` : This is a function that looks at the output and compares it to the expected output and returns a score. In our case, since we're comparing the recall and mrr at different levels of k, we'll be returning a list of scores.\n",
    "\n",
    "Since our task object is going to be running the retrieval with different methods, we'll also be redefining our `retrieve` function to take in additional parameters to be reconfigured for each experiment.\n",
    "\n",
    "We implemented (1) in the previous section. Now, we want to use a single function to retrieve the top k items from our database with different retrieval methods because it's significantly easier to modify a single function with configuration parameters than to keep track of individual functions for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braintrust import Score\n",
    "from lancedb.rerankers import CohereReranker\n",
    "import lancedb\n",
    "from typing import Literal\n",
    "\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "table = db.open_table(\"chunks\")\n",
    "# Configure a cohere reranker\n",
    "reranker = CohereReranker(model_name=\"rerank-multilingual-v3.0\", column=\"query\")\n",
    "\n",
    "\n",
    "def retrieve(\n",
    "    question: str,\n",
    "    max_k=25,\n",
    "    mode: Literal[\"vector\", \"fts\", \"hybrid\"] = \"vector\",\n",
    "    use_reranker: bool = False,\n",
    "):\n",
    "    results = table.search(question, query_type=mode).limit(max_k)\n",
    "    if use_reranker:\n",
    "        results = results.rerank(reranker=reranker)\n",
    "    return [result for result in results.to_list()]\n",
    "\n",
    "\n",
    "# Similar to our previous section, we can use the id of each item to compute the recall and MRR metrics.\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    predictions = [item[\"id\"] for item in output]\n",
    "    labels = [kwargs[\"metadata\"][\"chunk_id\"]]\n",
    "    return [\n",
    "        Score(\n",
    "            name=metric,\n",
    "            score=score_fn(predictions, labels),\n",
    "            metadata={\"query\": input, \"result\": output, **kwargs[\"metadata\"]},\n",
    "        )\n",
    "        for metric, score_fn in metrics.items()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: We will remove a chunk of these once braintrust is functional again. For now, we're manually logging and calculating the results\n",
    "\n",
    "We want to understand how each method performs when we vary the size of retrieve items with respect to recall and mrr. This helps us understand if a method is worth the cost and latency to implement. \n",
    "\n",
    "More practically, if a cheaper method has a recall@10 that's equivalent to a more expensive's method recall@5, then it might be worth it to start with the cheaper method first. Using these objective metrics, we can make an informed decision about which method to use. Let's see how we can do so with a simple example below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "# Define a class here to handle potential stalled retrieval requests - each request should take ~ 1-2 seconds at most so we can just kill the request after 5 seconds ( Noticed that some of my requests were stalling for lancedb)\n",
    "class Timeout:\n",
    "    def __init__(self, seconds=1, error_message='Timeout'):\n",
    "        self.seconds = seconds\n",
    "        self.error_message = error_message\n",
    "\n",
    "    def handle_timeout(self, signum, frame):\n",
    "        raise TimeoutError(self.error_message)\n",
    "\n",
    "    def __enter__(self):\n",
    "        signal.signal(signal.SIGALRM, self.handle_timeout)\n",
    "        signal.alarm(self.seconds)\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.alarm(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running with query mode vector and use reranker True: 100%|██████████| 290/290 [04:12<00:00,  1.15it/s]\n",
      "Running with query mode vector and use reranker False: 100%|██████████| 290/290 [02:34<00:00,  1.87it/s]\n",
      "Running with query mode fts and use reranker True: 100%|██████████| 290/290 [01:42<00:00,  2.82it/s]\n",
      "Running with query mode fts and use reranker False: 100%|██████████| 290/290 [00:04<00:00, 72.16it/s]\n",
      "Running with query mode hybrid and use reranker True: 100%|██████████| 290/290 [04:10<00:00,  1.16it/s]\n",
      "Running with query mode hybrid and use reranker False: 100%|██████████| 290/290 [02:39<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import braintrust\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "dataset = [\n",
    "    item\n",
    "    for item in braintrust.init_dataset(\n",
    "        project=\"Text-2-SQL\", name=\"Bird-Bench-Questions\"\n",
    "    )\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for query_mode, use_reranker in product([\"vector\", \"fts\", \"hybrid\"], [True, False]):\n",
    "    experiment_results = []\n",
    "    for data in tqdm(\n",
    "        dataset,\n",
    "        desc=f\"Running with query mode {query_mode} and use reranker {use_reranker}\",\n",
    "    ):\n",
    "        input_data = data[\"input\"]\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                with Timeout(seconds=5):\n",
    "                    retrieved_results = retrieve(\n",
    "                        input_data, 50, mode=query_mode, use_reranker=use_reranker\n",
    "                    )\n",
    "                    break\n",
    "            except TimeoutError:\n",
    "                continue\n",
    "        end_time = time.time()\n",
    "\n",
    "\n",
    "        predictions = [result[\"id\"] for result in retrieved_results]\n",
    "        labels = [data[\"metadata\"][\"chunk_id\"]]\n",
    "        scores = {\n",
    "            metric: score_fn(predictions, labels)\n",
    "            for metric, score_fn in metrics.items()\n",
    "        }\n",
    "        scores[\"avg_time\"] = end_time - start_time\n",
    "        experiment_results.append(scores)\n",
    "\n",
    "    results[(use_reranker, query_mode)] = pd.DataFrame(experiment_results)\n",
    "    # experiment.summarize(summarize_scores=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mrr@1</th>\n",
       "      <th>mrr@3</th>\n",
       "      <th>mrr@5</th>\n",
       "      <th>mrr@10</th>\n",
       "      <th>mrr@15</th>\n",
       "      <th>mrr@25</th>\n",
       "      <th>recall@1</th>\n",
       "      <th>recall@3</th>\n",
       "      <th>recall@5</th>\n",
       "      <th>recall@10</th>\n",
       "      <th>recall@15</th>\n",
       "      <th>recall@25</th>\n",
       "      <th>avg_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query parameter</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(True, 'vector')</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(False, 'vector')</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(True, 'fts')</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(False, 'fts')</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(True, 'hybrid')</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(False, 'hybrid')</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   mrr@1  mrr@3  mrr@5  mrr@10  mrr@15  mrr@25  recall@1  \\\n",
       "query parameter                                                            \n",
       "(True, 'vector')    0.60   0.68   0.69    0.70    0.70    0.70      0.60   \n",
       "(False, 'vector')   0.46   0.58   0.60    0.61    0.62    0.62      0.46   \n",
       "(True, 'fts')       0.55   0.58   0.59    0.59    0.59    0.60      0.55   \n",
       "(False, 'fts')      0.10   0.15   0.18    0.20    0.20    0.21      0.10   \n",
       "(True, 'hybrid')    0.60   0.67   0.68    0.69    0.69    0.70      0.60   \n",
       "(False, 'hybrid')   0.45   0.54   0.55    0.57    0.57    0.58      0.45   \n",
       "\n",
       "                   recall@3  recall@5  recall@10  recall@15  recall@25  \\\n",
       "query parameter                                                          \n",
       "(True, 'vector')       0.78      0.82       0.89       0.92       0.96   \n",
       "(False, 'vector')      0.73      0.79       0.91       0.95       0.98   \n",
       "(True, 'fts')          0.62      0.65       0.70       0.72       0.74   \n",
       "(False, 'fts')         0.22      0.33       0.46       0.52       0.61   \n",
       "(True, 'hybrid')       0.77      0.80       0.88       0.91       0.96   \n",
       "(False, 'hybrid')      0.64      0.72       0.83       0.89       0.94   \n",
       "\n",
       "                   avg_time  \n",
       "query parameter              \n",
       "(True, 'vector')       0.87  \n",
       "(False, 'vector')      0.53  \n",
       "(True, 'fts')          0.35  \n",
       "(False, 'fts')         0.01  \n",
       "(True, 'hybrid')       0.86  \n",
       "(False, 'hybrid')      0.55  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame with the mean values for all query parameters\n",
    "mean_results = pd.DataFrame(\n",
    "    {\n",
    "        \"query parameter\": [str(key) for key in results.keys()],\n",
    "        **{\n",
    "            metric: [results[key][metric].mean() for key in results.keys()]\n",
    "            for metric in results[list(results.keys())[0]].columns\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set 'query parameter' as the index\n",
    "mean_results.set_index(\"query parameter\", inplace=True)\n",
    "\n",
    "mean_results = mean_results.round(2)\n",
    "\n",
    "# Display the results as a table\n",
    "display(mean_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've computed the metrics above, we can see that\n",
    "\n",
    "1. A Re-Ranker improves recall@k for all query types, especially when k is small. For recall, this improvement is only seen for FTS and Hybrid Search. For embedding search, using a re-ranker resulted in slightly worse performance. This improvement comes at the cost of increased latency of around 300ms, which is huge for FTS.\n",
    "2. Embedding Search has around 3-13% higher recall than Hybrid Search for all values of k. The two have similar MRR@k for all values of k\n",
    "3. A Re-Ranker only improves MRR but not recall for embedding search, indicating that the additional latency might not be worth it if MRR is not a priority\n",
    "\n",
    "What's really surprising in this case is that embedding search alone outperforms hybrid search. Now that we've seen the individual performance, the next step is to do a sensitivity analysis. We do so because we want to make sure the differences we observe are robust and not due to chance. \n",
    "\n",
    "This is especially important if implementing these methods will require additional engineering effort. We'll explore some useful techniques such as bootstrapping and t-tests to do so in the next notebook `3. Visualising Results`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
