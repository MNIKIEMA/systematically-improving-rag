{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematically Improving Your Rag Application\n",
    "\n",
    "> If you haven't already, please run `1. Synthetic-Transactions.ipynb` to generate a dataset of synthetic transactions that you'll need to use for this notebook. You will need at least 256 examples to kick off a fine-tuning job.\n",
    "\n",
    "## Why start with a managed Re-Ranker?\n",
    "\n",
    "We want to start with a managed re-ranker early on, especially if we have a limited amount of data. This is because\n",
    "\n",
    "1. **Re-rankers are more data efficient** : In situations where data is scarce, re-rankers are very effective with a small amount of training examples. This is because unlike embedding models, they only need to learn how to optimize the ranking of a small set of candidates.\n",
    "\n",
    "2. **Easy to Integrate** : As more data becomes available, we can fine-tune our re-ranker or use it as a second step of our retrieval process. This allows us to progressively improve the quality of our results without having to overhaul our entire setup or re-embed all of our candidates from scratch. \n",
    "\n",
    "3. **Quick to Implement** : Using a hosted service like Cohere allows us to save valuable engineering time and resources. We don't have to worry about the underlying deployment infrastructure or focus on other considerations such as hyper-parameter tuning, context-length limitations etc which an open source model would require us to handle. This allows us to focus instead of experimenting and iterating quickly, enabling us to learn what works best for our application by looking at Recall and Mean Reciprocal Rank(MRR).\n",
    "\n",
    "In short, we recommend starting with a managed re-ranker early on because it's the fastest way to achieve the biggest performance improvements. We recommend using Cohere because they have one of the best re-ranker models in the industry right now and it integrates seamlessly with `lancedb`'s search api.\n",
    "\n",
    "\n",
    "## Why do we need Hard Negatives?\n",
    "\n",
    "In this notebook, we'll be mining for hard negatives by providing categories that are distinct yet challenging for the model to differentiate from the transaction's actual label. This approach helps the model understand subtle distinctions when ranking potential categories.\n",
    "\n",
    "We can see an example of this below where we have a query about Google , a relevant passage about Google Maps and an irrelevant passage about Napoli's famous dishes. We'd ideally want our model to rank the relevant passage higher than the irrelevant one.\n",
    "\n",
    "<img src=\"./data/hard_negatives.png\" width=\"600\"/>\n",
    "\n",
    "We want hard negatives because \n",
    "\n",
    "1. **Improves Discriminative Ability** : By providing the model with non-relevant examples, hard negatives train the re-ranker to discriminate between relevant examples and similar but irrelevant results. \n",
    "\n",
    "2. **Robustness to Noise** : Hard negatives allow our model to learn to ignore noisy results that are similar but irrelevant. This helps to improve the overall quality of our results.\n",
    "\n",
    "3. **Effecient use of limited data**: In situations where data is scarce, hard negatives allow us to make the most out of the limited data we have. This is because we can use a small amount of data to train our model to effectively distinguish between relevant and irrelevant results.\n",
    "\n",
    "More concretely, in our dataset, we would process each dataset item as follows.\n",
    "\n",
    "Query\n",
    "```\n",
    "Name : Ayden\n",
    "Category: Financial Software\n",
    "Department: Finance\n",
    "Location: Berlin, DE\n",
    "Amount: 1273.45\n",
    "Card: Enterprise Technology Services\n",
    "Trip Name: unknown \n",
    "```\n",
    "\n",
    "Relevant Passages\n",
    "```\n",
    "Subscription & Revenue Infrastructure\n",
    "```\n",
    "\n",
    "Hard Negatives\n",
    "```\n",
    "Office Equipment Maintenance\n",
    "Office Supplies & Stationery\n",
    "Human Resource\n",
    "```\n",
    "\n",
    "We're using a relatively simple approach here by randomly sampling other categories from our dataset. In practice, you might want to adopt a more sophisticated approach by using methods such as  \n",
    "\n",
    "1. **Cosine Similarity** : Only choosing negatives that have high similarity scores with the positive category\n",
    "2. **Adverserial Sampling** : Using language models to identify hard negatives and generate more off\n",
    "3. **K-Means Clustering** : Clustering categories and selecting negatives from the same cluster to increase discriminative ability of our re-ranker\n",
    "\n",
    "# Starting Our Finetuning process\n",
    "\n",
    "## Preparing our Dataset\n",
    "\n",
    "In our previous notebook, we initialized our dataset and split it into train/eval sets. We'll be using the training set here and mining for hard negatives to create a new finetuning dataset. \n",
    "\n",
    "We want to use `Pydantic` here to define a single instance of our finetuning data because it allows us to easily convert our data into the format that Cohere expects. Additionally, as our fine-tuning data gets more complex, it provides an easy way to add in custom validation logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braintrust import init_dataset\n",
    "\n",
    "dataset = init_dataset(project=\"fine-tuning\", name=\"Synthetic Transactions\")\n",
    "\n",
    "\n",
    "def get_dataset_split(split: str, dataset):\n",
    "    return [\n",
    "        {\n",
    "            \"input\": transaction[\"input\"],\n",
    "            \"expected\": transaction[\"expected\"],\n",
    "        }\n",
    "        for transaction in dataset\n",
    "        if transaction[\"metadata\"][\"split\"] == split\n",
    "    ]\n",
    "\n",
    "\n",
    "train_data = get_dataset_split(\"train\", dataset)\n",
    "eval_data = get_dataset_split(\"eval\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# Define Pydantic model to store our finetuning data\n",
    "class CohereFinetuneItem(BaseModel):\n",
    "    query: str\n",
    "    relevant_passages: list[str]\n",
    "    hard_negatives: list[str]\n",
    "\n",
    "\n",
    "# Get all the labels in our dataset \n",
    "labels = set([transaction[\"expected\"][0] for transaction in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "finetuning_data = []\n",
    "\n",
    "for transaction in train_data:\n",
    "    query = transaction[\"input\"]\n",
    "    relevant_passages = [transaction[\"expected\"][0]]\n",
    "\n",
    "    valid_hard_negatives = [label for label in labels if label != relevant_passages[0]]\n",
    "\n",
    "    for i in range(2):\n",
    "        hard_negatives = random.choices(valid_hard_negatives, k=4)\n",
    "        finetuning_data.append(\n",
    "            CohereFinetuneItem(\n",
    "                query=query,\n",
    "                relevant_passages=relevant_passages,\n",
    "                hard_negatives=hard_negatives,\n",
    "            )\n",
    "        )\n",
    "\n",
    "with open(\"./data/cohere_finetune.jsonl\", \"w\") as f:\n",
    "    for item in finetuning_data:\n",
    "        f.write(item.model_dump_json() + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Our Dataset\n",
    "\n",
    "A reminder that the fine-tuning itself will take around 1 hour to a day so you'll need to come back to this notebook later down the line when this is done.\n",
    "\n",
    "Once the dataset has a status `validated`, we can kick off our fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "...\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'validated'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "\n",
    "reranked_dataset = co.datasets.create(\n",
    "    name=\"Synthetic Transactions Finetune\",\n",
    "    data = open(\"./data/cohere_finetune.jsonl\",\"rb\"),\n",
    "    type=\"reranker-finetune-input\",\n",
    ")\n",
    "\n",
    "co.wait(reranked_dataset).dataset.validation_status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Our model\n",
    "\n",
    "Now that we've uploaded our dataset, we can kick off our fine-tuning job. Make sure to indicate that you're doing a re-ranker finetune when creating the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cohere.finetuning import BaseModel, FinetunedModel, Settings\n",
    "\n",
    "finetune_request = co.finetuning.create_finetuned_model(\n",
    "    request=FinetunedModel(\n",
    "        name=\"finetuned-cohere-reranker\",\n",
    "        settings=Settings(\n",
    "            base_model=BaseModel(base_type=\"BASE_TYPE_RERANK\"),\n",
    "            dataset_id=reranked_dataset.id,\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STATUS_READY'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = co.finetuning.get_finetuned_model(finetune_request.finetuned_model.id)\n",
    "response.finetuned_model.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking our Model\n",
    "\n",
    "We want to quantify the improvement that fine-tuning a model gets us. In order to do so, we'll be using the same retrieval evals as before to benchmark our fine-tuned model. Since we're building a model here that will suggest relevant categories for a given transaction, we'll be measuring the following two metrics.    \n",
    "\n",
    "- Recall : Whether the correct category is in the top N results\n",
    "- MRR : The mean reciprocal rank of the correct category in the top N results\n",
    "\n",
    "We want to mainly measure two things\n",
    "\n",
    "- How much of an improvement does a fine-tuned model get us over a pure embedding based approach\n",
    "- How does the fine-tuned model perform against the default Cohere re-ranker\n",
    "\n",
    "In order to do so, we'll be benchmarking our fine-tuned model against the default text-embedding-3-small model as well as the default Cohere re-ranker. We'll use `braintrust` here to run our evaluations and compare the results between our different configurations since it's where we've stored our evaluation data and provides an easy way to share our results with others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "# Connect to LanceDB and load in our pre-ingested categories table\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "table = db.open_table(\"categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment fine-tuning-1730888663 is running at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730888663\n",
      "fine-tuning (data): 66it [00:00, 40968.49it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cacd145d1c046428a6b04939f8116d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fine-tuning (tasks):   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "fine-tuning-1730888663 compared to fine-tuning-1730888635:\n",
      "72.73% 'mrr@1'    score\n",
      "81.31% (+31.57%) 'mrr@3'    score\t(32 improvements, 3 regressions)\n",
      "81.62% (+29.14%) 'mrr@5'    score\t(33 improvements, 3 regressions)\n",
      "72.73% 'recall@1' score\n",
      "90.91% (+28.79%) 'recall@3' score\t(19 improvements, 0 regressions)\n",
      "92.42% (+18.18%) 'recall@5' score\t(12 improvements, 0 regressions)\n",
      "\n",
      "3.89s (+79.16%) 'duration'\t(15 improvements, 51 regressions)\n",
      "\n",
      "See results for fine-tuning-1730888663 at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730888663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment fine-tuning-1730888676 is running at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730888676\n",
      "fine-tuning (data): 66it [00:00, 25122.43it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d97b98bf8fd4caa8ca8d07cb10c3f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fine-tuning (tasks):   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "fine-tuning-1730888676 compared to fine-tuning-1730888663:\n",
      "22.73% (-50.00%) 'mrr@1'    score\t(2 improvements, 35 regressions)\n",
      "25.76% (-55.56%) 'mrr@3'    score\t(3 improvements, 43 regressions)\n",
      "27.20% (-54.42%) 'mrr@5'    score\t(3 improvements, 44 regressions)\n",
      "22.73% (-50.00%) 'recall@1' score\t(2 improvements, 35 regressions)\n",
      "30.30% (-60.61%) 'recall@3' score\t(0 improvements, 40 regressions)\n",
      "36.36% (-56.06%) 'recall@5' score\t(0 improvements, 37 regressions)\n",
      "\n",
      "4.38s (+49.30%) 'duration'\t(7 improvements, 59 regressions)\n",
      "\n",
      "See results for fine-tuning-1730888676 at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730888676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment fine-tuning-1730888689 is running at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730888689\n",
      "fine-tuning (data): 66it [00:00, 24096.80it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81159273c1c04fc2841f6b885d30728a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fine-tuning (tasks):   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "fine-tuning-1730888689 compared to fine-tuning-1730888676:\n",
      "40.91% (+18.18%) 'mrr@1'    score\t(19 improvements, 7 regressions)\n",
      "49.75% (+23.99%) 'mrr@3'    score\t(28 improvements, 8 regressions)\n",
      "52.47% (+25.28%) 'mrr@5'    score\t(33 improvements, 8 regressions)\n",
      "40.91% (+18.18%) 'recall@1' score\t(19 improvements, 7 regressions)\n",
      "62.12% (+31.82%) 'recall@3' score\t(25 improvements, 4 regressions)\n",
      "74.24% (+37.88%) 'recall@5' score\t(26 improvements, 1 regressions)\n",
      "\n",
      "2.58s (-180.16%) 'duration'\t(64 improvements, 2 regressions)\n",
      "\n",
      "See results for fine-tuning-1730888689 at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730888689\n"
     ]
    }
   ],
   "source": [
    "from braintrust import Eval, Score\n",
    "import itertools\n",
    "from lancedb.rerankers import CohereReranker\n",
    "\n",
    "def calculate_mrr(predictions: list[str], gt: list[str]):\n",
    "    mrr = 0\n",
    "    for label in gt:\n",
    "        if label in predictions:\n",
    "            mrr = max(mrr, 1 / (predictions.index(label) + 1))\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def get_recall(predictions: list[str], gt: list[str]):\n",
    "    return len([label for label in gt if label in predictions]) / len(gt)\n",
    "\n",
    "\n",
    "eval_metrics = [[\"mrr\", calculate_mrr], [\"recall\", get_recall]]\n",
    "sizes = [1,3,5]\n",
    "\n",
    "metrics = {\n",
    "    f\"{metric_name}@{size}\": lambda predictions, gt, m=metric_fn, s=size: (\n",
    "        lambda p, g: m(p[:s], g)\n",
    "    )(predictions, gt)\n",
    "    for (metric_name, metric_fn), size in itertools.product(eval_metrics, sizes)\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    return [\n",
    "        Score(\n",
    "            name=metric,\n",
    "            score=score_fn(output, kwargs[\"expected\"]),\n",
    "            metadata={\"query\": input, \"result\": output, **kwargs[\"metadata\"]},\n",
    "        )\n",
    "        for metric, score_fn in metrics.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "def task(input,reranker):\n",
    "    query = table.search(input, query_type=\"vector\").limit(25)\n",
    "\n",
    "    if reranker:\n",
    "        query = query.rerank(reranker)\n",
    "    \n",
    "    return [\n",
    "        item[\"text\"]\n",
    "        for item in query.to_list()\n",
    "    ]\n",
    "\n",
    "\n",
    "rerankers = [\n",
    "    CohereReranker(model_name=f\"{finetune_request.finetuned_model.id}-ft\"),\n",
    "    CohereReranker(model_name=\"rerank-english-v3.0\"),\n",
    "    None,\n",
    "]\n",
    "\n",
    "for reranker in rerankers:\n",
    "    await Eval(\n",
    "        \"fine-tuning\",  # Replace with your project name\n",
    "        data=eval_data,\n",
    "        task=lambda query: task(query, reranker),  # Replace with your LLM call\n",
    "        scores=[evaluate_braintrust],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis and Next Steps\n",
    "\n",
    "| Metric | Text-Embedding-3-Small | Default Cohere Reranker | Fine-tuned Reranker |\n",
    "|--------|----------------------|-------------------------|---------------------|\n",
    "| MRR@1 | 0.41 | 0.23 (-44%) | 0.73 (+78%) |\n",
    "| MRR@3 | 0.50 | 0.26 (-48%) | 0.81 (+63%) |\n",
    "| MRR@5 | 0.52 | 0.27 (-48%) | 0.82 (+56%) |\n",
    "| Recall@1 | 0.41 | 0.23 (-44%) | 0.73 (+78%) |\n",
    "| Recall@3 | 0.62 | 0.30 (-51%) | 0.91 (+46%) |\n",
    "| Recall@5 | 0.74 | 0.36 (-51%) | 0.92 (+24%) |\n",
    "\n",
    "With just 256 examples, our fine-tuned reranker showed a 60%+ average increase in recall and a 49% average increase in mean reciprocal rank (MRR) compared to the text-embedding-3-small model. On the contrary, using the default Cohere re-ranker actually degraded performance at the cost of increased latency.\n",
    "\n",
    "Ultimately when you have a limited amount of data, fine-tuning a reranker is a no-brainer. It allows you to outperform the default model and get a good return on your investment. When looking to fine-tune it on your own data, some key considerations to look at are\n",
    "\n",
    "1. **Model Selection** : We chose to finetune on the cohere re-rankers but they have english and multi-lingual options to choose from. There are also other managed providers such as Jina that offer re-ranker models. It's important to experiment and find one that works for you\n",
    "2. **Dataset Quality** : In our example here, we mined for hard negatives by simply randomly selecting other categories. You might want to adopt a more sophisticated approach such as looking at cosine similarity or getting a language model involved.\n",
    "\n",
    "In the next notebook, we'll look at how we might fine-tune an open source model using the Sentence Transformers library. We'll need to manage our hyper-parameters, loss functions and training loop but at the same time, be able to squeeze out significantly more performance out of our model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
