{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematically Improving Your Rag Application\n",
    "\n",
    "We want to improve the quality of our expense categorization. To do so, we'll finetune a cohere reranker to do so.\n",
    "\n",
    "Since we only have ~80 examples, we'll bootstrap new examples by randomly sampling from our train dataset to find hard negatives from other categories.\n",
    "\n",
    "In order to fine-tune our dataset, we'll need to format it nicely for the cohere reranker. This means that we'll need to format our data in the following way\n",
    "\n",
    "```\n",
    "{\n",
    "    \"query\": \"What is the name of the merchant?\",\n",
    "    \"relevant_passages\": [\"McDonalds\", \"Starbucks\"],\n",
    "    \"hard_negatives\": [\"Exxon\", \"Chevron\"]\n",
    "}\n",
    "```\n",
    "\n",
    "In our case, we'll be using the `query` as the transaction input that we previously generated and the correct label as the `relevant_passages`. Other labels will be our `hard_negatives`.\n",
    "\n",
    "So for each example in our dataset, we can generate another 4 more by simply sampling from the other labels 4 more times to get 4 unique hard negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braintrust import init_dataset\n",
    "\n",
    "dataset = init_dataset(project=\"fine-tuning\", name=\"Synthetic Transactions\")\n",
    "\n",
    "\n",
    "def get_dataset_split(split: str, dataset):\n",
    "    return [\n",
    "        {\n",
    "            \"input\": transaction[\"input\"],\n",
    "            \"expected\": transaction[\"expected\"],\n",
    "        }\n",
    "        for transaction in dataset\n",
    "        if transaction[\"metadata\"][\"split\"] == split\n",
    "    ]\n",
    "\n",
    "\n",
    "train_data = get_dataset_split(\"train\", dataset)\n",
    "eval_data = get_dataset_split(\"eval\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class CohereFinetuneItem(BaseModel):\n",
    "    query: str\n",
    "    relevant_passages: list[str]\n",
    "    hard_negatives: list[str]\n",
    "\n",
    "\n",
    "labels = set([transaction[\"expected\"][0] for transaction in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "finetuning_data = []\n",
    "\n",
    "for transaction in train_data:\n",
    "    query = transaction[\"input\"]\n",
    "    relevant_passages = [transaction[\"expected\"][0]]\n",
    "\n",
    "    valid_hard_negatives = [label for label in labels if label != relevant_passages[0]]\n",
    "\n",
    "    for i in range(2):\n",
    "        hard_negatives = random.choices(valid_hard_negatives, k=4)\n",
    "        finetuning_data.append(\n",
    "            CohereFinetuneItem(\n",
    "                query=query,\n",
    "                relevant_passages=relevant_passages,\n",
    "                hard_negatives=hard_negatives,\n",
    "            )\n",
    "        )\n",
    "\n",
    "with open(\"./data/cohere_finetune.jsonl\", \"w\") as f:\n",
    "    for item in finetuning_data:\n",
    "        f.write(item.model_dump_json() + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "...\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'validated'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.ClientV2()\n",
    "\n",
    "reranked_dataset = co.datasets.create(\n",
    "    name=\"Synthetic Transactions Finetune\",\n",
    "    data = open(\"./data/cohere_finetune.jsonl\",\"rb\"),\n",
    "    type=\"reranker-finetune-input\",\n",
    ")\n",
    "\n",
    "co.wait(reranked_dataset).dataset.validation_status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cohere.finetuning import BaseModel, FinetunedModel, Settings\n",
    "\n",
    "finetune_request = co.finetuning.create_finetuned_model(\n",
    "    request=FinetunedModel(\n",
    "        name=\"finetuned-cohere-reranker\",\n",
    "        settings=Settings(\n",
    "            base_model=BaseModel(base_type=\"BASE_TYPE_RERANK\"),\n",
    "            dataset_id=reranked_dataset.id,\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STATUS_READY'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = co.finetuning.get_finetuned_model(finetune_request.finetuned_model.id)\n",
    "response.finetuned_model.status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "table = db.open_table(\"categories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment fine-tuning-1730888663 is running at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730888663\n",
      "fine-tuning (data): 66it [00:00, 40968.49it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cacd145d1c046428a6b04939f8116d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fine-tuning (tasks):   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "fine-tuning-1730888663 compared to fine-tuning-1730888635:\n",
      "72.73% 'mrr@1'    score\n",
      "81.31% (+31.57%) 'mrr@3'    score\t(32 improvements, 3 regressions)\n",
      "81.62% (+29.14%) 'mrr@5'    score\t(33 improvements, 3 regressions)\n",
      "72.73% 'recall@1' score\n",
      "90.91% (+28.79%) 'recall@3' score\t(19 improvements, 0 regressions)\n",
      "92.42% (+18.18%) 'recall@5' score\t(12 improvements, 0 regressions)\n",
      "\n",
      "3.89s (+79.16%) 'duration'\t(15 improvements, 51 regressions)\n",
      "\n",
      "See results for fine-tuning-1730888663 at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730888663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment fine-tuning-1730888676 is running at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730888676\n",
      "fine-tuning (data): 66it [00:00, 25122.43it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d97b98bf8fd4caa8ca8d07cb10c3f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fine-tuning (tasks):   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "fine-tuning-1730888676 compared to fine-tuning-1730888663:\n",
      "22.73% (-50.00%) 'mrr@1'    score\t(2 improvements, 35 regressions)\n",
      "25.76% (-55.56%) 'mrr@3'    score\t(3 improvements, 43 regressions)\n",
      "27.20% (-54.42%) 'mrr@5'    score\t(3 improvements, 44 regressions)\n",
      "22.73% (-50.00%) 'recall@1' score\t(2 improvements, 35 regressions)\n",
      "30.30% (-60.61%) 'recall@3' score\t(0 improvements, 40 regressions)\n",
      "36.36% (-56.06%) 'recall@5' score\t(0 improvements, 37 regressions)\n",
      "\n",
      "4.38s (+49.30%) 'duration'\t(7 improvements, 59 regressions)\n",
      "\n",
      "See results for fine-tuning-1730888676 at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730888676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment fine-tuning-1730888689 is running at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730888689\n",
      "fine-tuning (data): 66it [00:00, 24096.80it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81159273c1c04fc2841f6b885d30728a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fine-tuning (tasks):   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "fine-tuning-1730888689 compared to fine-tuning-1730888676:\n",
      "40.91% (+18.18%) 'mrr@1'    score\t(19 improvements, 7 regressions)\n",
      "49.75% (+23.99%) 'mrr@3'    score\t(28 improvements, 8 regressions)\n",
      "52.47% (+25.28%) 'mrr@5'    score\t(33 improvements, 8 regressions)\n",
      "40.91% (+18.18%) 'recall@1' score\t(19 improvements, 7 regressions)\n",
      "62.12% (+31.82%) 'recall@3' score\t(25 improvements, 4 regressions)\n",
      "74.24% (+37.88%) 'recall@5' score\t(26 improvements, 1 regressions)\n",
      "\n",
      "2.58s (-180.16%) 'duration'\t(64 improvements, 2 regressions)\n",
      "\n",
      "See results for fine-tuning-1730888689 at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730888689\n"
     ]
    }
   ],
   "source": [
    "from braintrust import Eval, Score\n",
    "import itertools\n",
    "from lancedb.rerankers import CohereReranker\n",
    "\n",
    "def calculate_mrr(predictions: list[str], gt: list[str]):\n",
    "    mrr = 0\n",
    "    for label in gt:\n",
    "        if label in predictions:\n",
    "            mrr = max(mrr, 1 / (predictions.index(label) + 1))\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def get_recall(predictions: list[str], gt: list[str]):\n",
    "    return len([label for label in gt if label in predictions]) / len(gt)\n",
    "\n",
    "\n",
    "eval_metrics = [[\"mrr\", calculate_mrr], [\"recall\", get_recall]]\n",
    "sizes = [1,3,5]\n",
    "\n",
    "metrics = {\n",
    "    f\"{metric_name}@{size}\": lambda predictions, gt, m=metric_fn, s=size: (\n",
    "        lambda p, g: m(p[:s], g)\n",
    "    )(predictions, gt)\n",
    "    for (metric_name, metric_fn), size in itertools.product(eval_metrics, sizes)\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    return [\n",
    "        Score(\n",
    "            name=metric,\n",
    "            score=score_fn(output, kwargs[\"expected\"]),\n",
    "            metadata={\"query\": input, \"result\": output, **kwargs[\"metadata\"]},\n",
    "        )\n",
    "        for metric, score_fn in metrics.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "def task(input,reranker):\n",
    "    query = table.search(input, query_type=\"vector\").limit(25)\n",
    "\n",
    "    if reranker:\n",
    "        query = query.rerank(reranker)\n",
    "    \n",
    "    return [\n",
    "        item[\"text\"]\n",
    "        for item in query.to_list()\n",
    "    ]\n",
    "\n",
    "\n",
    "rerankers = [\n",
    "    CohereReranker(model_name=f\"{finetune_request.finetuned_model.id}-ft\"),\n",
    "    CohereReranker(model_name=\"rerank-english-v3.0\"),\n",
    "    None,\n",
    "]\n",
    "\n",
    "for reranker in rerankers:\n",
    "    await Eval(\n",
    "        \"fine-tuning\",  # Replace with your project name\n",
    "        data=eval_data,\n",
    "        task=lambda query: task(query, reranker),  # Replace with your LLM call\n",
    "        scores=[evaluate_braintrust],\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
