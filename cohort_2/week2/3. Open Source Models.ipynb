{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systematically Improving RAG\n",
    "\n",
    "# When should we fine-tune an embedding model?\n",
    "\n",
    "> If you haven't, please make sure that you've ran `1. Synthethic Transactions.ipynb` and `2. Finetune Cohere.ipynb` first before proceeding with this section. This will ensure that you have all of the necessary data and models to follow along.\n",
    "\n",
    "In this notebook, we'll explore how to fine-tune an open source embedding model. We want to explore fine-tuning ideally when we have more data because we're training our model to learn a new mapping from our data. Ideally we'd start with fine-tuning the cross encoder first before considering fine-tuning a embedding model.\n",
    "\n",
    "We use `sentence-transformers` because it provides a large variety of loss functions out of the box and is compatible with Hugging Face, making it easy to upload and save the fine-tuned models when we're done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "In this section, we'll be fine-tuning the `BAAI/bge-base-en` model. We'll be using the `BatchSemiHardTripletLoss` loss function to train our model. \n",
    "\n",
    "\n",
    "A Batch Semi Hard Triplet Loss works by taking a batch of sentence pairs and computing the loss for all possible valid triplets, then identifying semi-hard positives and negatives. A semi-hard negative is an example that is not as close to the anchor as the positive example, but is still closeer to the anchor than the negative example. \n",
    "\n",
    "Using the original Cohere dataset format, \n",
    "\n",
    "- `query` : This would be the anchor here\n",
    "- `relevant_passages` : This would be the positive example\n",
    "- `hard_negatives` : This would be the negative example\n",
    "\n",
    "While a single training run can provide a baseline, exploring hyperparameter optimization—through techniques like grid or random search—can significantly enhance model performance, especially when executed on a larger scale with appropriate computational resources. Here's a quick example of [how we can do this using `modal` to run a grid search over all possible parameters](https://modal.com/blog/fine-tuning-embeddings)\n",
    "\n",
    "## Declaring Constants\n",
    "\n",
    "When writing fine-tuning code, we want to declare our constants up front. This ensures that we have a consistent set of parameters to use when training our model and makes it easy for us to change them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To resolve the warning from huggingface/tokenizers about parallelism:\n",
    "# 1. Avoid using `tokenizers` before the fork if possible.\n",
    "# 2. Explicitly set the environment variable TOKENIZERS_PARALLELISM to either 'true' or 'false'.\n",
    "import os\n",
    "\n",
    "# Set the environment variable to disable parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "BASE_MODEL_NAME = \"BAAI/bge-base-en\"\n",
    "FINETUNED_MODEL_NAME = \"ivanleomk/finetuned-bge-base-en\"\n",
    "\n",
    "MODEL_OUTPUT_DIR = \"./models/bge-base-en\"\n",
    "WANDB_RUN_NAME = \"bge-base-en\"\n",
    "CATEGORIES_PATH = \"data/categories.json\"\n",
    "TRAIN_EVALUATOR_NAME = \"bge-base-en-train\"\n",
    "EVAL_EVALUATOR_NAME = \"bge-base-en-eval\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "\n",
    "While we use the same dataset as before, we need to handle a lot more configuration when fine-tuning open source models in order for us to evaluate our model's performance while training as well as to use the `BatchSemiHardTripletLoss` loss function.\n",
    "\n",
    "To do so, we'll format our original train and eval split into a train, test and eval split. \n",
    "\n",
    "- Train : This is only used to train the model\n",
    "- Test : This is used to evaluate the model during training\n",
    "- Eval : This is used to evaluate the model after training\n",
    "\n",
    "We want to have a separate set of data points set aside for testing our model during training or to use when evaluating different versions of our model. This ensures that our model does not overfit to the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braintrust import init_dataset\n",
    "import json\n",
    "\n",
    "categories = json.load(open(CATEGORIES_PATH))\n",
    "dataset = init_dataset(project=\"fine-tuning\", name=\"Synthetic Transactions\")\n",
    "\n",
    "\n",
    "def get_dataset_split(split: str, dataset):\n",
    "    return [\n",
    "        {\n",
    "            \"input\": transaction[\"input\"],\n",
    "            \"expected\": transaction[\"expected\"],\n",
    "        }\n",
    "        for transaction in dataset\n",
    "        if transaction[\"metadata\"][\"split\"] == split\n",
    "    ]\n",
    "\n",
    "\n",
    "train_data = get_dataset_split(\"train\", dataset)\n",
    "eval_data = get_dataset_split(\"eval\", dataset)\n",
    "test_data = train_data[: int(len(train_data) * TEST_SIZE)]\n",
    "train_data = train_data[int(len(train_data) * TEST_SIZE) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def create_labels(data):\n",
    "    label_to_example = defaultdict(list)\n",
    "\n",
    "    for item in data:\n",
    "        label_to_example[item[\"expected\"][0]].append(item)\n",
    "\n",
    "    return {label: idx for idx, label in enumerate(label_to_example.keys())}\n",
    "\n",
    "\n",
    "def create_sentence_to_label_dataset(data, label_to_idx):\n",
    "    return Dataset.from_dict(\n",
    "        {\n",
    "            \"sentence\": [item[\"input\"] for item in data],\n",
    "            \"label\": [label_to_idx[item[\"expected\"][0]] for item in data],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def create_triplet_dataset(data):\n",
    "    label_to_example = defaultdict(list)\n",
    "\n",
    "    for item in data:\n",
    "        label_to_example[item[\"expected\"][0]].append(item)\n",
    "\n",
    "    labels = set(label_to_example.keys())\n",
    "\n",
    "    anchors = []\n",
    "    positives = []\n",
    "    negatives = []\n",
    "\n",
    "    for item in data:\n",
    "        label = item[\"expected\"][0]\n",
    "        anchor = item\n",
    "        positive = label\n",
    "        negative = random.choice([item for item in labels if item != label])\n",
    "        anchors.append(anchor)\n",
    "        positives.append(positive)\n",
    "        negatives.append(negative)\n",
    "\n",
    "    return {\"anchor\": anchors, \"positive\": positives, \"negative\": negatives}\n",
    "\n",
    "\n",
    "labels_to_idx = create_labels(train_data)\n",
    "\n",
    "train_triplets = create_triplet_dataset(train_data)\n",
    "test_triplets = create_triplet_dataset(test_data)\n",
    "eval_triplets = create_triplet_dataset(eval_data)\n",
    "\n",
    "sentence_to_label_train_dataset = create_sentence_to_label_dataset(\n",
    "    train_data, labels_to_idx\n",
    ")\n",
    "sentence_to_label_test_dataset = create_sentence_to_label_dataset(\n",
    "    test_data, labels_to_idx\n",
    ")\n",
    "sentence_to_label_eval_dataset = create_sentence_to_label_dataset(\n",
    "    eval_data, labels_to_idx\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "Now that we have our training data formatted in the right format, we can start training our model. We'll do so in 3 steps\n",
    "\n",
    "1. First we'll declare training arguments - we're using the default arguments provided in their documentation but ideally you'd want to experiment and tinkker with different configurations\n",
    "\n",
    "2. Next we'll start a training run. We're using the `wandb` integration here to log our training run to Weights & Biases. You'll need to create a Weights & Biases account and authenticate with `wandb login`\n",
    "\n",
    "3. Finally, we'll train our model before uploading it to the Hugging Face model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bge-base-en-train_cosine_accuracy': 0.8461538461538461,\n",
       " 'bge-base-en-train_dot_accuracy': 0.15384615384615385,\n",
       " 'bge-base-en-train_manhattan_accuracy': 0.8557692307692307,\n",
       " 'bge-base-en-train_euclidean_accuracy': 0.8461538461538461,\n",
       " 'bge-base-en-train_max_accuracy': 0.8557692307692307}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.losses import BatchSemiHardTripletLoss\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "\n",
    "model = SentenceTransformer(BASE_MODEL_NAME)\n",
    "loss = BatchSemiHardTripletLoss(model)\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=MODEL_OUTPUT_DIR,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # MultipleNegativesRankingLoss benefits from no duplicate samples in a batch\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    run_name=WANDB_RUN_NAME,  # Will be used in W&B if `wandb` is installed\n",
    ")\n",
    "\n",
    "train_evaluator = TripletEvaluator(\n",
    "    anchors=train_triplets[\"anchor\"],\n",
    "    positives=train_triplets[\"positive\"],\n",
    "    negatives=train_triplets[\"negative\"],\n",
    "    name=TRAIN_EVALUATOR_NAME,\n",
    ")\n",
    "\n",
    "train_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65/65 00:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7298c6753eb946fead20c28258f6a424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=65, training_loss=4.900086388221154, metrics={'train_runtime': 5.0757, 'train_samples_per_second': 204.897, 'train_steps_per_second': 12.806, 'total_flos': 0.0, 'train_loss': 4.900086388221154, 'epoch': 5.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=sentence_to_label_train_dataset,\n",
    "    eval_dataset=sentence_to_label_test_dataset,\n",
    "    loss=loss,\n",
    "    evaluator=train_evaluator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bge-base-en-eval_cosine_accuracy': 0.9545454545454546,\n",
       " 'bge-base-en-eval_dot_accuracy': 0.045454545454545456,\n",
       " 'bge-base-en-eval_manhattan_accuracy': 0.9545454545454546,\n",
       " 'bge-base-en-eval_euclidean_accuracy': 0.9545454545454546,\n",
       " 'bge-base-en-eval_max_accuracy': 0.9545454545454546}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluator = TripletEvaluator(\n",
    "    anchors=eval_triplets[\"anchor\"],\n",
    "    positives=eval_triplets[\"positive\"],\n",
    "    negatives=eval_triplets[\"negative\"],\n",
    "    name=EVAL_EVALUATOR_NAME,\n",
    ")\n",
    "test_evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/ivanleomk/finetuned-bge-base-en/commit/c3c83d0594e980f60275019f1cb16fba8738d27d'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(f\"models/finetuned-{BASE_MODEL_NAME}\")\n",
    "model.push_to_hub(FINETUNED_MODEL_NAME, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Here we use `lancedb` again to evaluate our model. It comes with out of the box support for hugging face models, which makkes it incredibly easy for us to evaluate our fine-tuned model vs the base model. \n",
    "\n",
    "We'll similarly use `braintrust` to evaluate our model and compare it against the base model by looking at recall and mrr @1,3,5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "import json\n",
    "\n",
    "def create_lancedb_table(model_name: str, categories: list[str]):\n",
    "    model = get_registry().get(\"huggingface\").create(name=model_name)\n",
    "\n",
    "\n",
    "    class Category(LanceModel):\n",
    "        text: str = model.SourceField()\n",
    "        embedding: Vector(model.ndims()) = model.VectorField()\n",
    "\n",
    "\n",
    "    db = lancedb.connect(\"./lancedb\")\n",
    "    table = db.create_table(\n",
    "        f\"categories-{model_name.replace('/', '-')}\", schema=Category, mode=\"overwrite\"\n",
    "    )\n",
    "\n",
    "    table.add(\n",
    "        [\n",
    "            {\n",
    "                \"text\": category[\"category\"],\n",
    "            }\n",
    "            for category in categories\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def calculate_mrr(predictions: list[str], gt: list[str]):\n",
    "    mrr = 0\n",
    "    for label in gt:\n",
    "        if label in predictions:\n",
    "            mrr = max(mrr, 1 / (predictions.index(label) + 1))\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def get_recall(predictions: list[str], gt: list[str]):\n",
    "    return len([label for label in gt if label in predictions]) / len(gt)\n",
    "\n",
    "\n",
    "eval_metrics = [[\"mrr\", calculate_mrr], [\"recall\", get_recall]]\n",
    "sizes = [1,3,5]\n",
    "\n",
    "metrics = {\n",
    "    f\"{metric_name}@{size}\": lambda predictions, gt, m=metric_fn, s=size: (\n",
    "        lambda p, g: m(p[:s], g)\n",
    "    )(predictions, gt)\n",
    "    for (metric_name, metric_fn), size in itertools.product(eval_metrics, sizes)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-11T12:06:05Z WARN  lance::dataset] No existing dataset at /Users/ivanleo/Documents/coding/systematically-improving-rag/cohort_2/week2/lancedb/categories-BAAI-bge-base-en.lance, it will be created\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7c17c68e774b6daf1ada1dde7513a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39704a19775f435390392a70e4cd13d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f355772d774265a817c12d973478b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6f58f8e7b4465cb3192581f4b95706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c845a34fd3fd4bb2ad6cf10f6137ff5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89b3176609e4ca5a4f525d2f919e216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-11T12:06:36Z WARN  lance::dataset] No existing dataset at /Users/ivanleo/Documents/coding/systematically-improving-rag/cohort_2/week2/lancedb/categories-ivanleomk-finetuned-bge-base-en.lance, it will be created\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/lancedb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m base_table \u001b[38;5;241m=\u001b[39m create_lancedb_table(BASE_MODEL_NAME, categories)\n\u001b[1;32m      5\u001b[0m finetuned_table \u001b[38;5;241m=\u001b[39m create_lancedb_table(FINETUNED_MODEL_NAME, categories)\n\u001b[0;32m----> 7\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mlancedb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/root/lancedb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_braintrust\u001b[39m(\u001b[38;5;28minput\u001b[39m, output, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     12\u001b[0m         Score(\n\u001b[1;32m     13\u001b[0m             name\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m metric, score_fn \u001b[38;5;129;01min\u001b[39;00m metrics\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     18\u001b[0m     ]\n",
      "File \u001b[0;32m~/Documents/coding/systematically-improving-rag/cohort_2/week2/.venv/lib/python3.11/site-packages/lancedb/__init__.py:115\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(uri, api_key, region, host_override, read_consistency_interval, request_thread_pool, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown keyword arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLanceDBConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_consistency_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_consistency_interval\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/coding/systematically-improving-rag/cohort_2/week2/.venv/lib/python3.11/site-packages/lancedb/db.py:339\u001b[0m, in \u001b[0;36mLanceDBConnection.__init__\u001b[0;34m(self, uri, read_consistency_interval)\u001b[0m\n\u001b[1;32m    337\u001b[0m         uri \u001b[38;5;241m=\u001b[39m Path(uri)\n\u001b[1;32m    338\u001b[0m     uri \u001b[38;5;241m=\u001b[39m uri\u001b[38;5;241m.\u001b[39mexpanduser()\u001b[38;5;241m.\u001b[39mabsolute()\n\u001b[0;32m--> 339\u001b[0m     \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(uri)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_entered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/pathlib.py:1120\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   1119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmkdir(mode, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.10-macos-aarch64-none/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/root'"
     ]
    }
   ],
   "source": [
    "\n",
    "from braintrust import Score, Eval\n",
    "\n",
    "categories = json.load(open(CATEGORIES_PATH))\n",
    "base_table = create_lancedb_table(BASE_MODEL_NAME, categories)\n",
    "finetuned_table = create_lancedb_table(FINETUNED_MODEL_NAME, categories)\n",
    "\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "\n",
    "\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    return [\n",
    "        Score(\n",
    "            name=metric,\n",
    "            score=score_fn(output, kwargs[\"expected\"]),\n",
    "            metadata={\"query\": input, \"result\": output, **kwargs[\"metadata\"]},\n",
    "        )\n",
    "        for metric, score_fn in metrics.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "def task(user_query, table_to_query):\n",
    "    return [\n",
    "        item[\"text\"]\n",
    "        for item in table_to_query.search(user_query, query_type=\"vector\")\n",
    "        .limit(25)\n",
    "        .to_list()\n",
    "    ]\n",
    "\n",
    "for query_table in [base_table, finetuned_table]:\n",
    "    await Eval(\n",
    "        \"fine-tuning\",\n",
    "        experiment_name=f\"synthetic-transactions-train-{query_table.name}\",\n",
    "        data=lambda: eval_data,\n",
    "        task=lambda query: task(query, query_table),\n",
    "        scores=[evaluate_braintrust],\n",
    "        metadata={\"model\": query_table.name},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Through this three-part series, we've explored different approaches to systematically improving RAG systems through fine-tuning.\n",
    "\n",
    "1. We first explored how to bootstrap a synthetic data from scratch iteratively, leveraging manual annotation and prompting to iteratively improve our model.\n",
    "2. We then looked at why you might want to consider looking at crossencoders first when data is scarce and why a managed provider like Cohere might be a better starting point than training a model from scratch.\n",
    "3. Lastly, we explored how fine-tuning an open source embedding model with the `sentence-transformers` library was an easy way to get started but required significantly more low level configuration and hyperparameter tuning.\n",
    "\n",
    "\n",
    "The results from our fine-tuning are telling. \n",
    "\n",
    "| Metric | Base Model | Fine-tuned Model | Improvement |\n",
    "|-----------|------------|------------------|-------------|\n",
    "| MRR@1 | 0.36 | 0.56 | +19.70% |\n",
    "| MRR@3 | 0.49 | 0.69 | +20.45% |\n",
    "| MRR@5 | 0.51 | 0.71 | +19.17% |\n",
    "| Recall@1 | 0.36 | 0.56 | +19.70% |\n",
    "| Recall@3 | 0.68 | 0.86 | +18.18% |\n",
    "| Recall@5 | 0.79 | 0.91 | +12.12% |\n",
    "\n",
    "While managed solutions like Cohere offer faster time-to-production and simplified deployment, open-source models provide greater control and customization potential. The choice between these approaches should be guided by your specific requirements, resources, and the level of performance improvement needed for your use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
