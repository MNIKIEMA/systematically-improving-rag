{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 : Systematically Improving Your Rag Application\n",
    "\n",
    "# Why Fine-tune Embeddings?\n",
    "\n",
    "> If you have not already, please make sure that you've gone through Week 1's notebooks. That'll help to contextualise this week's notebook.\n",
    "\n",
    "Fine-Tuning a model allows a model to learn the unique nuances and patterns in your data. We can use smaller specialised models that can provide better performance at a significantly reduced cost than a general purpose model. \n",
    "\n",
    "With just 100 examples, we were able to beat proprietary models like OpenAI's `text-embedding-3-small` and achieve greater accuracy at a lower price point with open source embedding models such as `bge-base-en-v1.5` ( See [Fine Tuning Embeddings With Modal](https://modal.com/blog/fine-tuning-embeddings)) on the Quora dataset. \n",
    "\n",
    "For most production applications, that's less than **1-2 weeks of data** to get a massive improvement in accuracy. \n",
    "\n",
    "[Ramp](https://engineering.ramp.com/transaction-embeddings) fine-tuned an embedding model on transaction data to automatically suggest expense categories. Even though each customer's expense categories were unique, their model accurately generalized to new customers. This showcases how fine-tuning can adapt models to specific, real-world tasks.\n",
    "\n",
    "## What you'll achieve in this notebook?\n",
    "\n",
    "We're going to replicate Ramp's process using synthetic financial transaction data that we'll generate from scratch. This will give a step by step guide to fine-tune an embedding model for your own applications, whether it's through a proprietary model provider like Cohere or an open source embedding model. \n",
    "\n",
    "We have defined 25 categories ahead of time that we'll use to classify our synthetic transactions. Each transaction should map to a single category. We want to measure the ability of a model to identify the correct category for each transaction using just embeddings.\n",
    "\n",
    "This will be done in 3 main steps\n",
    "\n",
    "1. **Understand Transaction Dataset** : We'll learn about the type of data used by Ramp to fine-tune their embedding model and how we can replicate that with synthetic data using `instructor`.\n",
    "\n",
    "2. **Iterate on Synthetic Data** : We'll then iteratively generate a large dataset of transactions by generating examples with a language model and then selecting the best examples manually using a streamlit dataset. For each new batch of transactions, we'll verify that these are challenging examples by evaluating the recall performance of a baseline with our initial questions.\n",
    "\n",
    "3. **Evaluate and Fine-Tune Models** : Once we've generated enough questions, we'll then split our data into a train and evaluation set that we can use to evaluate the performance improvements of fine-tuning an embedding model. We'll do so with both Cohere's reranker models and an open source embedding model.\n",
    "\n",
    "Throughout this process, we'll use `braintrust`[https://braintrust.dev] to collect data and measure improvements. Braintrust makes it easy to collaborate with a team and simplifies data collection and evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Transactions\n",
    "\n",
    "To fine-tune our model effectively, we need to understand the transaction data we're working with.\n",
    "\n",
    "Typical Transaction Fields:\n",
    "\n",
    "- Merchant Name: The vendor or service provider's name.\n",
    "- Merchant Category Code (MCC): General category of the transaction (e.g., Restaurants).\n",
    "- Department Name: The company department responsible for the transaction.\n",
    "- Location: Where the transaction took place.\n",
    "- Amount: The transaction's monetary value.\n",
    "- Spend Program Name: Specific budget or spend limit allocated.\n",
    "- Trip Name: If the transaction occurred during travel.\n",
    "\n",
    "We can see an example below\n",
    "\n",
    "```\n",
    "Name : Beirut Bakery\n",
    "Category: Restaurants, Cafeteria\n",
    "Department: Engineering\n",
    "Location: Calgary, AB, Canada\n",
    "Amount: 56.67 CAD\n",
    "Card: Ramp's Physical Card\n",
    "Trip Name: unknown\n",
    "```\n",
    "\n",
    "This is a difficult task because there's very little information. \n",
    "\n",
    "Additionally since each company has unique categories that have some implicit rules, it's difficult for a general embedding model to classify these transactions without fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Synthetic Transactions\n",
    "\n",
    "Since we don't have a dataset of transactions to work with, we'll need to generate our own. We use `instructor `here because it makes it easy to switch between different models and iterate on the prompt with the jinja templating. We're using `gpt-4o-mini` here because it's cheap to use, making it perfect for getting practice with labelling and generating examples.\n",
    "\n",
    "We'll be iteratively generating this dataset in 3 steps\n",
    "\n",
    "1. **Data Generation** : We'll first generate an iniital batch of synthetic transactions. These will be generated using a simple prompt and we will be using a streamlit application which you can run using `streamlit run label.py` to manually review and select the best examples. We recommend using `ChatGPT` or `Claude` to reason and evaluate these examples to get a sense for what makes a good or bad example during this process.\n",
    "\n",
    "2. **Data Refinement** : We'll then use these initial examples to generate new examples that are more challenging by including a random subset of these examples in the prompt. The important step here is to use these examples as a way to find characteristics and patterns of good and bad examples that we can then add to our prompt as rules or few shot examples to guide the model to generate better examples.\n",
    "\n",
    "3. **Braintrust Evaluation**: We'll ingest in the categories into `lancedb` and use `braintrust` to log the recall@1,3,5 and mrr@1,3,5 of our initial and refined examples. We use `lancedb` because it provides a single api to access re-rankers, vector search and batching of embeddings for us. This makes it easy for us to experiment with different retrieval configurations ( Eg. Vector Search, Re-Rankers ) easily.\n",
    "\n",
    "We'll repeat this process until we've generated at least 300 examples. This ensures that we have enough examples to fine-tune a cohere re-ranker ( requires min 256 examples) or a sentence transformer model while also having enough examples to create a held out evaluation set.\n",
    "\n",
    "We want to iterate on our synthetic data for two main reasons.\n",
    "\n",
    "1. By generating a small amount of data, it becomes practical to manually label examples and slowly build up a high quality dataset of examples.  We can ensure that each example is challenging by calculating recall and mrr at different subsets of the retrieved data using braintrust as we iteratively generate better examples.\n",
    "\n",
    "2. By constantly sampling a random sample of an ever growing number of examples, we're able to introduce randomness in our prompt that can create a diverse dataset. This helps us to avoid potential issues with diversity and quality that doing a single pass of data generation can introduce.\n",
    "\n",
    "\n",
    "## Step 1 : Generating our initial transactions.\n",
    "\n",
    "We'll start by generating our initial transactions using a simple prompt. These are going to be very simple examples that will not be great but they are useful as an initial starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator, ValidationInfo\n",
    "from openai import AsyncOpenAI\n",
    "import instructor\n",
    "from typing import Optional\n",
    "from textwrap import dedent\n",
    "import random\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "# Load in pre-defined categories\n",
    "categories = json.load(open(\"data/categories.json\"))\n",
    "\n",
    "# Define a Pydantic model that can represent the same transaction data that Ramp was using\n",
    "class Transaction(BaseModel):\n",
    "    merchant_name: str\n",
    "    merchant_category: list[str]\n",
    "    department: str\n",
    "    location: str\n",
    "    amount: float\n",
    "    spend_program_name: str\n",
    "    trip_name: Optional[str] = None\n",
    "    expense_category: str\n",
    "\n",
    "    def generate_transaction(self):\n",
    "        return dedent(f\"\"\"\n",
    "        Name : {self.merchant_name}\n",
    "        Category: {\", \".join(self.merchant_category)}\n",
    "        Department: {self.department}\n",
    "        Location: {self.location}\n",
    "        Amount: {self.amount}\n",
    "        Card: {self.spend_program_name}\n",
    "        Trip Name: {self.trip_name if self.trip_name else \"unknown\"}\n",
    "        \"\"\")\n",
    "\n",
    "    @field_validator(\"expense_category\")\n",
    "    @classmethod\n",
    "    def validate_expense_category(cls, v, info: ValidationInfo):\n",
    "        if not info.context or not info.context[\"category\"]:\n",
    "            return v\n",
    "        return info.context[\"category\"][\"category\"]\n",
    "\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "async def generate_transaction(category):\n",
    "    return await client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Generate a transaction for a tech company that could be filed under the category of {{ category }}. This should be distinct from the sample_transactions provided in the categories.json file\n",
    "\n",
    "                - The spend program is a specific spending authority or allocation that has defined limits, rules, and permissions. It's like a virtual card or spending account set up for a specific purpose.\n",
    "                - Merchant Category Name is a label that best describes the merchant of the transaction.\n",
    "                - Merchant name should be realistic and not obviously made up.\n",
    "                \"\"\",\n",
    "            }\n",
    "        ],\n",
    "        context={\"category\": category},\n",
    "        response_model=Transaction,\n",
    "    )\n",
    "\n",
    "\n",
    "# Generate 5 initial transactions and choose the category randomly\n",
    "coros = []\n",
    "for _ in range(5):\n",
    "    coros.append(generate_transaction(random.choice(categories)))\n",
    "\n",
    "transactions = await asyncio.gather(*coros)\n",
    "with open(\"./data/generated_transactions.jsonl\", \"a\") as f:\n",
    "    for transaction in transactions:\n",
    "        f.write(transaction.model_dump_json() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Labeling Transactions\n",
    "\n",
    "Now that we've generated a small set of initial transactions, please run `streamlit run label.py` to manually select transactions that you think might be difficult to classify. \n",
    "\n",
    "> You can modify and edit the transaction details before approving them. Hot keys of ctrl + e ( approve ) and ctrl + r ( reject ) make this process much faster. Only approved transactions will be saved to `generated_transactions.jsonl` below. We'll then use these examples to generate a new set of transactions that are more challenging.\n",
    "\n",
    "You can also manually override transaction details in the streamlit application. We recommend using `ChatGPT` or `Claude` to discuss and generate good default and examples. A prompt that I used to prompt the chat UI was\n",
    "\n",
    "```\n",
    "I'd like to generate a transaction for a tech company that is challenging to classify into a specific category. Here are the details\n",
    "\n",
    "<Details go here>\n",
    "\n",
    "I'd like you to help rewrite some of the details to make it more realistic. Please stick to the following rules\n",
    "\n",
    "- MCCs should be realistic. If possible, let's try to use a MCC that will cover a superset of the given category\n",
    "- Let's try to suggest a non-uniform number (Eg. not 1500 ) so that it seems more realistic\n",
    "- The Spend Program name should be a specific spending authority or allocation that has defined limits, rules, and permissions. It's like a virtual card or spending account set up for a specific purpose. In our case, this spend program name should not be a name that directly mentions the category or merchant\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_transaction_with_examples(category, examples: list[Transaction]):\n",
    "    return await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "                Generate a potentially ambiguous business transaction that could reasonably be categorized as {{ category }} or another similar category. The goal is to create transactions that challenge automatic categorization systems by having characteristics that could fit multiple categories.\n",
    "\n",
    "\n",
    "                Available categories in the system.:\n",
    "                <categories>\n",
    "                {% for category_option in categories %}\n",
    "                    {{ category_option[\"category\"] }}\n",
    "                {% endfor %}\n",
    "                </categories>\n",
    "\n",
    "                \n",
    "                The transaction should:\n",
    "                1. Use a realistic but non-obvious merchant name (international names welcome), don't use names that are obviously made u \n",
    "                2. Include a plausible but non-rounded amount with decimals (e.g., $1247.83)\n",
    "                3. Be difficult to categorize definitively (could fit in multiple categories)\n",
    "                4. Merchant Category Name(s) should not reference the category at all and should be able to be used for other similar categories if possible.\n",
    "\n",
    "                Here are some good examples of transactions that were previously generated for other categories.\n",
    "\n",
    "                {% for example in examples %}\n",
    "                {{ example.model_dump_json() }}\n",
    "                {% endfor %}\n",
    "                \"\"\",\n",
    "            }\n",
    "        ],\n",
    "        context={\"category\": category, \"examples\": examples, \"categories\": categories},\n",
    "        response_model=Transaction,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/cleaned.jsonl\", \"r\") as f:\n",
    "    sample_transactions = []\n",
    "    for line in f:\n",
    "        sample_transactions.append(Transaction(**json.loads(line)))\n",
    "\n",
    "\n",
    "coros = []\n",
    "for _ in range(20):\n",
    "    coros.append(generate_transaction_with_examples(random.choice(categories), random.sample(sample_transactions, 10)))\n",
    "\n",
    "transactions = await asyncio.gather(*coros)\n",
    "\n",
    "with open(\"./data/generated_transactions.jsonl\", \"w\") as f:\n",
    "    for transaction in transactions:\n",
    "        f.write(transaction.model_dump_json() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Evaluating Recall and MRR Performance.\n",
    "\n",
    "Remember that we're building a model that can suggest transaction categories to a user. To do so, we'll only be able to show the top 3-5 results and we want to make sure that the correct result is ranked as highly as possible.\n",
    "\n",
    "Therefore, we'll be using recall and mrr to evaluate our mode's performance here. \n",
    "\n",
    "- `recall` : This measures whether the correct category is in the top k retrieved results.\n",
    "- `mrr` : This measures how highly is the correct category ranked in the retrieved results.\n",
    "\n",
    "Ideally we want a model with a high recall and mrr. This means that when we display the results, they're likely to be relevant to the user. By measuring the recall and mrr, we're able to ensure that we're conssitently generating questions that the model finds challenging. \n",
    "\n",
    "We're using `lancedb` here since it provides an easy way to perform these evaluations with automatic batching of embeddings for queries and data along with a single api for vector search and reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n",
    "categories = json.load(open(\"data/categories.json\"))\n",
    "\n",
    "class Category(LanceModel):\n",
    "    text: str = func.SourceField()\n",
    "    embedding: Vector(func.ndims()) = func.VectorField()\n",
    "\n",
    "\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "table = db.create_table(\"categories\", schema=Category, mode=\"overwrite\")\n",
    "\n",
    "\n",
    "table.add(\n",
    "    [\n",
    "        {\n",
    "            \"text\": category[\"category\"],\n",
    "        }\n",
    "        for category in categories\n",
    "    ]\n",
    ")\n",
    "\n",
    "table.create_fts_index(field_names=[\"text\"], replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment fine-tuning-1730883986 is running at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730883986\n",
      "fine-tuning (data): 326it [00:00, 67959.40it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a957117d56ec43e494f762c6f68916b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fine-tuning (tasks):   0%|          | 0/326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "fine-tuning-1730883986 compared to fine-tuning-1730883924:\n",
      "38.34% (-) 'mrr@1'    score\t(0 improvements, 0 regressions)\n",
      "49.28% (-) 'mrr@3'    score\t(0 improvements, 0 regressions)\n",
      "52.14% (-) 'mrr@5'    score\t(0 improvements, 0 regressions)\n",
      "38.34% (-) 'recall@1' score\t(0 improvements, 0 regressions)\n",
      "63.80% (-) 'recall@3' score\t(0 improvements, 0 regressions)\n",
      "76.38% (-) 'recall@5' score\t(0 improvements, 0 regressions)\n",
      "\n",
      "8.22s (-54.64%) 'duration'\t(190 improvements, 56 regressions)\n",
      "\n",
      "See results for fine-tuning-1730883986 at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730883986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from braintrust import Eval, Score\n",
    "import itertools\n",
    "\n",
    "transactions = []\n",
    "for line in open(\"./data/cleaned.jsonl\").readlines():\n",
    "    transactions.append(Transaction(**json.loads(line)))\n",
    "\n",
    "len(transactions)\n",
    "\n",
    "def calculate_mrr(predictions: list[str], gt: list[str]):\n",
    "    mrr = 0\n",
    "    for label in gt:\n",
    "        if label in predictions:\n",
    "            mrr = max(mrr, 1 / (predictions.index(label) + 1))\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def get_recall(predictions: list[str], gt: list[str]):\n",
    "    return len([label for label in gt if label in predictions]) / len(gt)\n",
    "\n",
    "\n",
    "eval_metrics = [[\"mrr\", calculate_mrr], [\"recall\", get_recall]]\n",
    "sizes = [1,3,5]\n",
    "\n",
    "metrics = {\n",
    "    f\"{metric_name}@{size}\": lambda predictions, gt, m=metric_fn, s=size: (\n",
    "        lambda p, g: m(p[:s], g)\n",
    "    )(predictions, gt)\n",
    "    for (metric_name, metric_fn), size in itertools.product(eval_metrics, sizes)\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    return [\n",
    "        Score(\n",
    "            name=metric,\n",
    "            score=score_fn(output, kwargs[\"expected\"]),\n",
    "            metadata={\"query\": input, \"result\": output, **kwargs[\"metadata\"]},\n",
    "        )\n",
    "        for metric, score_fn in metrics.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "def task(user_query):\n",
    "    return [\n",
    "        item[\"text\"]\n",
    "        for item in table.search(user_query, query_type=\"vector\")\n",
    "        .limit(25)\n",
    "        .to_list()\n",
    "    ]\n",
    "\n",
    "\n",
    "await Eval(\n",
    "    \"fine-tuning\",  # Replace with your project name\n",
    "    data=lambda: [\n",
    "        {\n",
    "            \"input\": transaction.generate_transaction(),\n",
    "            \"expected\": [transaction.expense_category],\n",
    "        }\n",
    "        for transaction in transactions\n",
    "    ],  # Replace with your eval dataset\n",
    "    task=task,  # Replace with your LLM call\n",
    "    scores=[evaluate_braintrust],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we've generated a large dataset of synthetic transactions that we can use to fine-tune a model on. However, it's important here to call out that synthetic data has its challenges.\n",
    "\n",
    "1. Quality and Diversity : It's difficult to ensure that the synethic data is of high quality and diverse. We've done so by manually reviewing and selecting good examples but ultimately we need real production data to ensure that our model is able to generalise.\n",
    "\n",
    "2. Human Error : Manual review is great to ensure the quality of transactions but is expensive and error prone. This is not something that scales well, especially if you're trying to generate thousands of examples which you'd like humans to manually label.\n",
    "\n",
    "We want to treat this synthetic data as a starting point and iteratively make it better using the techniques we've discussed in this notebook. But you will need to eventually mix in production data and continue generating synthetic data in order to adequately evaluate and test the generalisation capabilities of your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dataset\n",
    "\n",
    "At this point, we've generated a large dataset of synthetic transactions that we can use to fine-tune a model on. \n",
    "\n",
    "We want to segregate our data into a train and evaluation set because it allows us to evaluate the performance of our model on data that it hasn't seen before. We use `braintrust` here to upload our dataset and a simple metadata flag to segregate between a train and evaluation portion of our dataset. This allows us to easily run evaluations on our model in the subsequent notebooks later on.\n",
    "\n",
    "If we fine-tuned our model on the same data that we evaluated it on, it would be difficult to tell if the improvements we made were due to the model generalizing better or due to overfitting. In this case, we're just going to split our data by randomly shuffling it and then selecting the first 80% as our training set and the remaining 20% as our evaluation set.\n",
    "\n",
    "In practice, you'd want to think carefully about these splits - using the category as a way to ensure that we have a diverse set of examples or generating new labels for the evaluation set based on the training labels. (Eg. Restaurants -> Dining Establishments or randomly grouping categories together )\n",
    "\n",
    "Before we start fine-tuning our models here, we also need to make sure that the evaluation set and training set are similar. We do so by measuring the recall and mrr and verifying that they have similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetSummary</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">project_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fine-tuning'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">dataset_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Synthetic Transactions'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">project_url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://www.braintrust.dev/app/567/p/fine-tuning'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">dataset_url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://www.braintrust.dev/app/567/p/fine-tuning/datasets/Synthetic%20Transactions'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">data_summary</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DataSummary</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">new_records</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">326</span>, <span style=\"color: #808000; text-decoration-color: #808000\">total_records</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">326</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDatasetSummary\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mproject_name\u001b[0m=\u001b[32m'fine-tuning'\u001b[0m,\n",
       "    \u001b[33mdataset_name\u001b[0m=\u001b[32m'Synthetic Transactions'\u001b[0m,\n",
       "    \u001b[33mproject_url\u001b[0m=\u001b[32m'https://www.braintrust.dev/app/567/p/fine-tuning'\u001b[0m,\n",
       "    \u001b[33mdataset_url\u001b[0m=\u001b[32m'https://www.braintrust.dev/app/567/p/fine-tuning/datasets/Synthetic%20Transactions'\u001b[0m,\n",
       "    \u001b[33mdata_summary\u001b[0m=\u001b[1;35mDataSummary\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnew_records\u001b[0m=\u001b[1;36m326\u001b[0m, \u001b[33mtotal_records\u001b[0m=\u001b[1;36m326\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from braintrust import init_dataset\n",
    "\n",
    "train_ratio = 0.8 * len(transactions)\n",
    "\n",
    "random.shuffle(transactions)\n",
    "\n",
    "train_transactions = transactions[:int(train_ratio)]\n",
    "eval_transactions = transactions[int(train_ratio):]\n",
    "\n",
    "dataset = init_dataset(project=\"fine-tuning\", name=\"Synthetic Transactions\")\n",
    "\n",
    "for transaction in train_transactions:\n",
    "    dataset.insert(\n",
    "        input=transaction.generate_transaction(),\n",
    "        expected=[transaction.expense_category],\n",
    "        metadata={\"split\": \"train\"},\n",
    "    )\n",
    "\n",
    "for transaction in eval_transactions:\n",
    "    dataset.insert(\n",
    "        input=transaction.generate_transaction(),\n",
    "        expected=[transaction.expense_category],\n",
    "        metadata={\"split\": \"eval\"},\n",
    "    )\n",
    "\n",
    "print(dataset.summarize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can get a baseline performance for each individual split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 66)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dataset_split(split:str, dataset):\n",
    "    return [\n",
    "        {\n",
    "            \"input\": transaction['input'],\n",
    "            \"expected\": transaction['expected'],\n",
    "        }\n",
    "        for transaction in dataset\n",
    "        if transaction[\"metadata\"][\"split\"] == split\n",
    "    ]\n",
    "\n",
    "train_data = get_dataset_split(\"train\", dataset)\n",
    "eval_data = get_dataset_split(\"eval\", dataset)\n",
    "len(train_data), len(eval_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment synthetic-transactions-train-04a0fc30 is running at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/synthetic-transactions-train-04a0fc30\n",
      "fine-tuning [experiment_name=synthetic-transactions-train] (data): 260it [00:00, 53451.58it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2c6f70322d4203b32606ae3f464a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fine-tuning [experiment_name=synthetic-transactions-train] (tasks):   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "synthetic-transactions-train-04a0fc30 compared to fine-tuning-1730883986:\n",
      "37.69% (-) 'mrr@1'    score\t(0 improvements, 0 regressions)\n",
      "49.17% (-) 'mrr@3'    score\t(0 improvements, 0 regressions)\n",
      "52.05% (-) 'mrr@5'    score\t(0 improvements, 0 regressions)\n",
      "37.69% (-) 'recall@1' score\t(0 improvements, 0 regressions)\n",
      "64.23% (-) 'recall@3' score\t(0 improvements, 0 regressions)\n",
      "76.92% (-) 'recall@5' score\t(0 improvements, 0 regressions)\n",
      "\n",
      "6.65s (-125.99%) 'duration'\t(145 improvements, 115 regressions)\n",
      "\n",
      "See results for synthetic-transactions-train-04a0fc30 at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/synthetic-transactions-train-04a0fc30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Eval(\n",
    "    \"fine-tuning\",\n",
    "    experiment_name=\"synthetic-transactions-train\",\n",
    "    data=lambda: train_data,\n",
    "    task=task,  # Replace with your LLM call\n",
    "    scores=[evaluate_braintrust],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment synthetic-transactions-train-cb6b07e2 is running at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/synthetic-transactions-train-cb6b07e2\n",
      "fine-tuning [experiment_name=synthetic-transactions-train] (data): 66it [00:00, 26946.76it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8da2774617a412980dddec8432792b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fine-tuning [experiment_name=synthetic-transactions-train] (tasks):   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "synthetic-transactions-train-cb6b07e2 compared to synthetic-transactions-train-04a0fc30:\n",
      "40.91% 'mrr@1'    score\n",
      "49.75% 'mrr@3'    score\n",
      "52.47% 'mrr@5'    score\n",
      "40.91% 'recall@1' score\n",
      "62.12% 'recall@3' score\n",
      "74.24% 'recall@5' score\n",
      "\n",
      "1.67s duration\n",
      "\n",
      "See results for synthetic-transactions-train-cb6b07e2 at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/synthetic-transactions-train-cb6b07e2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Eval(\n",
    "    \"fine-tuning\",\n",
    "    experiment_name=\"synthetic-transactions-train\",\n",
    "    data=lambda: eval_data,\n",
    "    task=task,  # Replace with your LLM call\n",
    "    scores=[evaluate_braintrust],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Analysis\n",
    "\n",
    "We can see that the performance of the text-embedding-3-small model is similar between the training and evaluation set. \n",
    "\n",
    "| Metric | Training Set (n=260) | Evaluation Set (n=66) | Difference |\n",
    "|--------|---------------------|----------------------|------------|\n",
    "| Recall@1 | 0.37 | 0.41 | -4% |\n",
    "| Recall@3 | 0.64 | 0.62 | -2% |\n",
    "| Recall@5 | 0.77 | 0.74 | -3% |\n",
    "| MRR@1 | 0.38 | 0.41 | -3% |\n",
    "| MRR@3 | 0.49 | 0.50 | -1% |\n",
    "| MRR@5 | 0.52 | 0.53 | -1% |\n",
    "\n",
    "\n",
    "In our initial experiment, we implemented a straightforward evaluation approach by randomly shuffling the dataset and creating an 80-20 split, resulting in 260 training examples and 66 evaluation examples. The performance metrics shown above demonstrate remarkably consistent behavior between the training and evaluation sets, with differences typically around 2-3%. This consistency suggests that both sets are drawn from the same underlying distribution, providing a solid foundation for model training and evaluation.\n",
    "\n",
    "While these results are promising for our proof-of-concept, it's important to note that a production implementation would benefit from more sophisticated validation strategies. \n",
    "\n",
    "This could include\n",
    "\n",
    "1. Generating synonymous labels to test if the model is able to generalise to other similar labels\n",
    "2. Implementing multiple validation sets with different characteristics\n",
    "3. Carefully managing data composition to prevent leakage\n",
    "\n",
    "These enhancements are out of the scope of this notebook and we'll leave that for a production implementation. In the meantime, let's move on to using cohere's re-ranker models to fine-tune our embedding model.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
