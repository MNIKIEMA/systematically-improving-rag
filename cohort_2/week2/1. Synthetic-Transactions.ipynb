{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 : Systematically Improving Your Rag Application\n",
    "\n",
    "# Why Fine-tune Embeddings?\n",
    "\n",
    "> If you have not already, please make sure that you've gone through Week 1's notebooks. That'll help to contextualise this week's notebook.\n",
    "\n",
    "Fine-Tuning a model allows a model to learn the unique nuances and patterns in your data. We can use smaller specialised models that can provide better performance at a significantly reduced cost than a general purpose model. \n",
    "\n",
    "With just 100 examples, we were able to beat proprietary models like OpenAI's `text-embedding-3-small` and achieve greater accuracy at a lower price point with open source embedding models such as `bge-base-en-v1.5` ( See [Fine Tuning Embeddings With Modal](https://modal.com/blog/fine-tuning-embeddings)) on the Quora dataset. \n",
    "\n",
    "For most production applications, that's less than **1-2 weeks of data** to get a massive improvement in accuracy. \n",
    "\n",
    "[Ramp](https://engineering.ramp.com/transaction-embeddings) fine-tuned an embedding model on transaction data to automatically suggest expense categories. Even though each customer's expense categories were unique, their model accurately generalized to new customers. This showcases how fine-tuning can adapt models to specific, real-world tasks.\n",
    "\n",
    "## What you'll achieve in this notebook?\n",
    "\n",
    "We're going to replicate Ramp's process using synthetic financial transaction data that we'll generate from scratch. This will give a step by step guide to fine-tune an embedding model for your own applications, whether it's through a proprietary model provider like Cohere or an open source embedding model. \n",
    "\n",
    "We have defined 25 categories ahead of time that we'll use to classify our synthetic transactions. Each transaction should map to a single category. We want to measure the ability of a model to identify the correct category for each transaction using just embeddings.\n",
    "\n",
    "This will be done in 4 main steps\n",
    "\n",
    "1. **Understand Transaction Dataset** : We'll learn about the type of data used by Ramp to fine-tune their embedding model and how we can replicate that with synthetic data using `instructor`.\n",
    "\n",
    "2. **Iterate on Synthetic Data** : We'll then iteratively generate a large dataset of transactions by generating examples with a language model and then selecting the best examples manually using a streamlit dataset. \n",
    "\n",
    "For each new batch of transactions, we'll verify that these are challenging examples by evaluating the recall performance of a baseline with our initial questions.\n",
    "\n",
    "3. **Evaluate and Fine-Tune Models** : Once we've generated enough questions, we'll then split our data into a train and evaluation set that we can use to evaluate the performance improvements of fine-tuning an embedding model. We'll do so with both Cohere's reranker models and an open source embedding model.\n",
    "\n",
    "Throughout this process, we'll use `braintrust`[https://braintrust.dev] to collect data and measure improvements. Braintrust makes it easy to collaborate with a team and simplifies data collection and evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Transactions\n",
    "\n",
    "To fine-tune our model effectively, we need to understand the transaction data we're working with.\n",
    "\n",
    "Typical Transaction Fields:\n",
    "\n",
    "- Merchant Name: The vendor or service provider's name.\n",
    "- Merchant Category Code (MCC): General category of the transaction (e.g., Restaurants).\n",
    "- Department Name: The company department responsible for the transaction.\n",
    "- Location: Where the transaction took place.\n",
    "- Amount: The transaction's monetary value.\n",
    "- Spend Program Name: Specific budget or spend limit allocated.\n",
    "- Trip Name: If the transaction occurred during travel.\n",
    "\n",
    "We can see an example below\n",
    "\n",
    "```\n",
    "Name : Beirut Bakery\n",
    "Category: Restaurants, Cafeteria\n",
    "Department: Engineering\n",
    "Location: Calgary, AB, Canada\n",
    "Amount: 56.67 CAD\n",
    "Card: Ramp's Physical Card\n",
    "Trip Name: unknown\n",
    "```\n",
    "\n",
    "This is a difficult task because there's very little information. \n",
    "\n",
    "Additionally since each company has unique categories that have some implicit rules, it's difficult for a general embedding model to classify these transactions without fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Synthetic Transactions\n",
    "\n",
    "## Step 1 : Why do we want to iterate on our synthetic data?\n",
    "\n",
    "We want to iterate on our synthetic data for two main reasons.\n",
    "\n",
    "1. By generating a small amount of data, it becomes practical to manually label examples and slowly build up a high quality dataset of examples. We can ensure that each example is challenging by calculating recall and mrr at different subsets of the retrieved data using braintrust as we iteratively generate better examples.\n",
    "\n",
    "2. By constantly sampling a random sample of an ever growing number of examples, we're able to introduce randomness in our prompt that can create a diverse dataset. This helps us to avoid potential issues with diversity and quality that doing a single pass of data generation can introduce.\n",
    "\n",
    "We'll iterate on our synthetic data in 3 steps\n",
    "\n",
    "1. We'll load in the pre-defined categories and then use `gpt-4o-mini` to generate a small set of initial transactions. We'll save these transactions to `data/generated_transactions.jsonl`.\n",
    "2. We'll then use a streamlit app at `label.py` to manually select transactions that we think might be difficult to classify. We'll then sample from these examples to generate a new set of transactions.\n",
    "3. Once we've got a new set of examples, we'll evaluate the recall performance of these new examples with respect to an initial baseline to ensure that these new examples are challenging.\n",
    "\n",
    "We'll repeat this process until we've generated at least 300 examples. This ensures that we have enough examples to fine-tune a cohere re-ranker ( requires min 256 examples) or a sentence transformer model while also having enough examples to create a held out evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, field_validator, ValidationInfo\n",
    "from openai import AsyncOpenAI\n",
    "import instructor\n",
    "from typing import Optional\n",
    "from textwrap import dedent\n",
    "import random\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "# Load in pre-defined categories\n",
    "categories = json.load(open(\"data/categories.json\"))\n",
    "\n",
    "# Define a Pydantic model that can represent the same transaction data that Ramp was using\n",
    "class Transaction(BaseModel):\n",
    "    merchant_name: str\n",
    "    merchant_category: list[str]\n",
    "    department: str\n",
    "    location: str\n",
    "    amount: float\n",
    "    spend_program_name: str\n",
    "    trip_name: Optional[str] = None\n",
    "    expense_category: str\n",
    "\n",
    "    def generate_transaction(self):\n",
    "        return dedent(f\"\"\"\n",
    "        Name : {self.merchant_name}\n",
    "        Category: {\", \".join(self.merchant_category)}\n",
    "        Department: {self.department}\n",
    "        Location: {self.location}\n",
    "        Amount: {self.amount}\n",
    "        Card: {self.spend_program_name}\n",
    "        Trip Name: {self.trip_name if self.trip_name else \"unknown\"}\n",
    "        \"\"\")\n",
    "\n",
    "    @field_validator(\"expense_category\")\n",
    "    @classmethod\n",
    "    def validate_expense_category(cls, v, info: ValidationInfo):\n",
    "        if not info.context or not info.context[\"category\"]:\n",
    "            return v\n",
    "        return info.context[\"category\"][\"category\"]\n",
    "\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "async def generate_transaction(category):\n",
    "    return await client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"Generate a transaction for a tech company that could be filed under the category of {{ category }}. This should be distinct from the sample_transactions provided in the categories.json file\n",
    "\n",
    "                - The spend program is a specific spending authority or allocation that has defined limits, rules, and permissions. It's like a virtual card or spending account set up for a specific purpose.\n",
    "                - Merchant Category Name is a label that best describes the merchant of the transaction.\n",
    "                - Merchant name should be realistic and not obviously made up.\n",
    "                \"\"\",\n",
    "            }\n",
    "        ],\n",
    "        context={\"category\": category},\n",
    "        response_model=Transaction,\n",
    "    )\n",
    "\n",
    "\n",
    "# Generate 5 initial transactions and choose the category randomly\n",
    "coros = []\n",
    "for _ in range(5):\n",
    "    coros.append(generate_transaction(random.choice(categories)))\n",
    "\n",
    "transactions = await asyncio.gather(*coros)\n",
    "with open(\"./data/generated_transactions.jsonl\", \"a\") as f:\n",
    "    for transaction in transactions:\n",
    "        f.write(transaction.model_dump_json() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Labeling Transactions\n",
    "\n",
    "Now that we've generated a small set of initial transactions, please run `streamlit run label.py` to manually select transactions that you think might be difficult to classify. \n",
    "\n",
    "You can modify and edit the transaction details before approving them. Hot keys of ctrl + e ( approve ) and ctrl + r ( reject ) make this process much faster. Only approved transactions will be saved to `generated_transactions.jsonl` below. We'll then use these examples to generate a new set of transactions that are more challenging.\n",
    "\n",
    "Your goal here is to generate around 10-20 examples that would be challenging for a model to classify. Once you've done so, please proceed on to braintrust below to evaluate the recall and mrr of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_transaction_with_examples(category, examples: list[Transaction]):\n",
    "    return await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "                Generate a potentially ambiguous business transaction that could reasonably be categorized as {{ category }} or another similar category. The goal is to create transactions that challenge automatic categorization systems by having characteristics that could fit multiple categories.\n",
    "\n",
    "\n",
    "                Available categories in the system.:\n",
    "                <categories>\n",
    "                {% for category_option in categories %}\n",
    "                    {{ category_option[\"category\"] }}\n",
    "                {% endfor %}\n",
    "                </categories>\n",
    "\n",
    "                \n",
    "                The transaction should:\n",
    "                1. Use a realistic but non-obvious merchant name (international names welcome), don't use names that are obviously made u \n",
    "                2. Include a plausible but non-rounded amount with decimals (e.g., $1247.83)\n",
    "                3. Be difficult to categorize definitively (could fit in multiple categories)\n",
    "                4. Merchant Category Name(s) should not reference the category at all and should be able to be used for other similar categories if possible.\n",
    "\n",
    "                Here are some good examples of transactions that were previously generated for other categories.\n",
    "\n",
    "                {% for example in examples %}\n",
    "                {{ example.model_dump_json() }}\n",
    "                {% endfor %}\n",
    "                \"\"\",\n",
    "            }\n",
    "        ],\n",
    "        context={\"category\": category, \"examples\": examples, \"categories\": categories},\n",
    "        response_model=Transaction,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/cleaned.jsonl\", \"r\") as f:\n",
    "    sample_transactions = []\n",
    "    for line in f:\n",
    "        sample_transactions.append(Transaction(**json.loads(line)))\n",
    "\n",
    "\n",
    "coros = []\n",
    "for _ in range(40):\n",
    "    coros.append(generate_transaction_with_examples(random.choice(categories), random.sample(sample_transactions, 10)))\n",
    "\n",
    "transactions = await asyncio.gather(*coros)\n",
    "\n",
    "with open(\"./data/generated_transactions.jsonl\", \"w\") as f:\n",
    "    for transaction in transactions:\n",
    "        f.write(transaction.model_dump_json() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Evaluating Recall Performance\n",
    "\n",
    "We want to evaluate the recall and mrr of our model at different subsets of the retrieved data. We'll do so by creating a `braintrust` project. We want to use `lancedb` to store our categories because it handles batching of our embeddings and provides a single api for us to experiment with different retrieval configurations ( Eg. Vector Search, Re-Rankers ) easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "\n",
    "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n",
    "categories = json.load(open(\"data/categories.json\"))\n",
    "\n",
    "class Category(LanceModel):\n",
    "    text: str = func.SourceField()\n",
    "    embedding: Vector(func.ndims()) = func.VectorField()\n",
    "\n",
    "\n",
    "db = lancedb.connect(\"./lancedb\")\n",
    "table = db.create_table(\"categories\", schema=Category, mode=\"overwrite\")\n",
    "\n",
    "\n",
    "table.add(\n",
    "    [\n",
    "        {\n",
    "            \"text\": category[\"category\"],\n",
    "        }\n",
    "        for category in categories\n",
    "    ]\n",
    ")\n",
    "\n",
    "table.create_fts_index(field_names=[\"text\"], replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment fine-tuning-1730883986 is running at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730883986\n",
      "fine-tuning (data): 326it [00:00, 67959.40it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a957117d56ec43e494f762c6f68916b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fine-tuning (tasks):   0%|          | 0/326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "fine-tuning-1730883986 compared to fine-tuning-1730883924:\n",
      "38.34% (-) 'mrr@1'    score\t(0 improvements, 0 regressions)\n",
      "49.28% (-) 'mrr@3'    score\t(0 improvements, 0 regressions)\n",
      "52.14% (-) 'mrr@5'    score\t(0 improvements, 0 regressions)\n",
      "38.34% (-) 'recall@1' score\t(0 improvements, 0 regressions)\n",
      "63.80% (-) 'recall@3' score\t(0 improvements, 0 regressions)\n",
      "76.38% (-) 'recall@5' score\t(0 improvements, 0 regressions)\n",
      "\n",
      "8.22s (-54.64%) 'duration'\t(190 improvements, 56 regressions)\n",
      "\n",
      "See results for fine-tuning-1730883986 at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/fine-tuning-1730883986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from braintrust import Eval, Score\n",
    "import itertools\n",
    "\n",
    "transactions = []\n",
    "for line in open(\"./data/cleaned.jsonl\").readlines():\n",
    "    transactions.append(Transaction(**json.loads(line)))\n",
    "\n",
    "len(transactions)\n",
    "\n",
    "def calculate_mrr(predictions: list[str], gt: list[str]):\n",
    "    mrr = 0\n",
    "    for label in gt:\n",
    "        if label in predictions:\n",
    "            mrr = max(mrr, 1 / (predictions.index(label) + 1))\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def get_recall(predictions: list[str], gt: list[str]):\n",
    "    return len([label for label in gt if label in predictions]) / len(gt)\n",
    "\n",
    "\n",
    "eval_metrics = [[\"mrr\", calculate_mrr], [\"recall\", get_recall]]\n",
    "sizes = [1,3,5]\n",
    "\n",
    "metrics = {\n",
    "    f\"{metric_name}@{size}\": lambda predictions, gt, m=metric_fn, s=size: (\n",
    "        lambda p, g: m(p[:s], g)\n",
    "    )(predictions, gt)\n",
    "    for (metric_name, metric_fn), size in itertools.product(eval_metrics, sizes)\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate_braintrust(input, output, **kwargs):\n",
    "    return [\n",
    "        Score(\n",
    "            name=metric,\n",
    "            score=score_fn(output, kwargs[\"expected\"]),\n",
    "            metadata={\"query\": input, \"result\": output, **kwargs[\"metadata\"]},\n",
    "        )\n",
    "        for metric, score_fn in metrics.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "def task(user_query):\n",
    "    return [\n",
    "        item[\"text\"]\n",
    "        for item in table.search(user_query, query_type=\"vector\")\n",
    "        .limit(25)\n",
    "        .to_list()\n",
    "    ]\n",
    "\n",
    "\n",
    "await Eval(\n",
    "    \"fine-tuning\",  # Replace with your project name\n",
    "    data=lambda: [\n",
    "        {\n",
    "            \"input\": transaction.generate_transaction(),\n",
    "            \"expected\": [transaction.expense_category],\n",
    "        }\n",
    "        for transaction in transactions\n",
    "    ],  # Replace with your eval dataset\n",
    "    task=task,  # Replace with your LLM call\n",
    "    scores=[evaluate_braintrust],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dataset\n",
    "\n",
    "We want to segregate our data into a train and evaluation set because it allows us to evaluate the performance of our model on data that it hasn't seen before. We use `braintrust` here to upload our dataset and a simple metadata flag to segregate between a train and evaluation portion of our dataset. This allows us to easily run evaluations on our model in the subsequent notebooks later on.\n",
    "\n",
    "If we fine-tuned our model on the same data that we evaluated it on, it would be difficult to tell if the improvements we made were due to the model generalizing better or due to overfitting. In this case, we're just going to split our data by randomly shuffling it and then selecting the first 80% as our training set and the remaining 20% as our evaluation set.\n",
    "\n",
    "In practice, you'd want to think carefully about these splits - using the category as a way to ensure that we have a diverse set of examples or generating new labels for the evaluation set based on the training labels. (Eg. Restaurants -> Dining Establishments or randomly grouping categories together )\n",
    "\n",
    "Before we start fine-tuning our models here, we also need to make sure that the evaluation set and training set are similar. We do so by measuring the recall and mrr and verifying that they have similar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetSummary</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">project_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fine-tuning'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">dataset_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Synthetic Transactions'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">project_url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://www.braintrust.dev/app/567/p/fine-tuning'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">dataset_url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://www.braintrust.dev/app/567/p/fine-tuning/datasets/Synthetic%20Transactions'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">data_summary</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DataSummary</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">new_records</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">326</span>, <span style=\"color: #808000; text-decoration-color: #808000\">total_records</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">326</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDatasetSummary\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mproject_name\u001b[0m=\u001b[32m'fine-tuning'\u001b[0m,\n",
       "    \u001b[33mdataset_name\u001b[0m=\u001b[32m'Synthetic Transactions'\u001b[0m,\n",
       "    \u001b[33mproject_url\u001b[0m=\u001b[32m'https://www.braintrust.dev/app/567/p/fine-tuning'\u001b[0m,\n",
       "    \u001b[33mdataset_url\u001b[0m=\u001b[32m'https://www.braintrust.dev/app/567/p/fine-tuning/datasets/Synthetic%20Transactions'\u001b[0m,\n",
       "    \u001b[33mdata_summary\u001b[0m=\u001b[1;35mDataSummary\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnew_records\u001b[0m=\u001b[1;36m326\u001b[0m, \u001b[33mtotal_records\u001b[0m=\u001b[1;36m326\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from braintrust import init_dataset\n",
    "\n",
    "train_ratio = 0.8 * len(transactions)\n",
    "\n",
    "random.shuffle(transactions)\n",
    "\n",
    "train_transactions = transactions[:int(train_ratio)]\n",
    "eval_transactions = transactions[int(train_ratio):]\n",
    "\n",
    "dataset = init_dataset(project=\"fine-tuning\", name=\"Synthetic Transactions\")\n",
    "\n",
    "for transaction in train_transactions:\n",
    "    dataset.insert(\n",
    "        input=transaction.generate_transaction(),\n",
    "        expected=[transaction.expense_category],\n",
    "        metadata={\"split\": \"train\"},\n",
    "    )\n",
    "\n",
    "for transaction in eval_transactions:\n",
    "    dataset.insert(\n",
    "        input=transaction.generate_transaction(),\n",
    "        expected=[transaction.expense_category],\n",
    "        metadata={\"split\": \"eval\"},\n",
    "    )\n",
    "\n",
    "print(dataset.summarize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if we can get a baseline performance for each individual split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 66)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_dataset_split(split:str, dataset):\n",
    "    return [\n",
    "        {\n",
    "            \"input\": transaction['input'],\n",
    "            \"expected\": transaction['expected'],\n",
    "        }\n",
    "        for transaction in dataset\n",
    "        if transaction[\"metadata\"][\"split\"] == split\n",
    "    ]\n",
    "\n",
    "train_data = get_dataset_split(\"train\", dataset)\n",
    "eval_data = get_dataset_split(\"eval\", dataset)\n",
    "len(train_data), len(eval_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment synthetic-transactions-train-04a0fc30 is running at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/synthetic-transactions-train-04a0fc30\n",
      "fine-tuning [experiment_name=synthetic-transactions-train] (data): 260it [00:00, 53451.58it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2c6f70322d4203b32606ae3f464a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fine-tuning [experiment_name=synthetic-transactions-train] (tasks):   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "synthetic-transactions-train-04a0fc30 compared to fine-tuning-1730883986:\n",
      "37.69% (-) 'mrr@1'    score\t(0 improvements, 0 regressions)\n",
      "49.17% (-) 'mrr@3'    score\t(0 improvements, 0 regressions)\n",
      "52.05% (-) 'mrr@5'    score\t(0 improvements, 0 regressions)\n",
      "37.69% (-) 'recall@1' score\t(0 improvements, 0 regressions)\n",
      "64.23% (-) 'recall@3' score\t(0 improvements, 0 regressions)\n",
      "76.92% (-) 'recall@5' score\t(0 improvements, 0 regressions)\n",
      "\n",
      "6.65s (-125.99%) 'duration'\t(145 improvements, 115 regressions)\n",
      "\n",
      "See results for synthetic-transactions-train-04a0fc30 at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/synthetic-transactions-train-04a0fc30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Eval(\n",
    "    \"fine-tuning\",\n",
    "    experiment_name=\"synthetic-transactions-train\",\n",
    "    data=lambda: train_data,\n",
    "    task=task,  # Replace with your LLM call\n",
    "    scores=[evaluate_braintrust],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment synthetic-transactions-train-cb6b07e2 is running at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/synthetic-transactions-train-cb6b07e2\n",
      "fine-tuning [experiment_name=synthetic-transactions-train] (data): 66it [00:00, 26946.76it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8da2774617a412980dddec8432792b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fine-tuning [experiment_name=synthetic-transactions-train] (tasks):   0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================SUMMARY=========================\n",
      "synthetic-transactions-train-cb6b07e2 compared to synthetic-transactions-train-04a0fc30:\n",
      "40.91% 'mrr@1'    score\n",
      "49.75% 'mrr@3'    score\n",
      "52.47% 'mrr@5'    score\n",
      "40.91% 'recall@1' score\n",
      "62.12% 'recall@3' score\n",
      "74.24% 'recall@5' score\n",
      "\n",
      "1.67s duration\n",
      "\n",
      "See results for synthetic-transactions-train-cb6b07e2 at https://www.braintrust.dev/app/567/p/fine-tuning/experiments/synthetic-transactions-train-cb6b07e2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalResultWithSummary(summary=\"...\", results=[...])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await Eval(\n",
    "    \"fine-tuning\",\n",
    "    experiment_name=\"synthetic-transactions-train\",\n",
    "    data=lambda: eval_data,\n",
    "    task=task,  # Replace with your LLM call\n",
    "    scores=[evaluate_braintrust],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we've seen how we can use synthetic data to generate an evaluation set that we can use to evaluate the performance of our model. We've also seen how we can use braintrust to evaluate the recall and mrr of our model at different subsets of the retrieved data.\n",
    "\n",
    "Now in the next notebook, we'll see how fine-tuning can improve the performance of our model to select the correct category for each transaction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
